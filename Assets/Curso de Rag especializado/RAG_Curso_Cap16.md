# Cap√≠tulo 16: C√≥mo se Entrenan los Embedding Models

---

## El Trabajo del Embedding Model

> "El trabajo es simple de describir: embeber texto similar a vectores cercanos, y texto diferente a vectores lejanos."

### Pero si lo pens√°s...

```
¬øC√≥mo puede una computadora ENTENDER el significado de un texto?

Es una haza√±a incre√≠blemente sofisticada.
```

---

## Pares Positivos y Negativos

### El concepto:

```
PAR POSITIVO (deben estar CERCA):
‚îú‚îÄ‚îÄ "good morning"
‚îî‚îÄ‚îÄ "hello"
(significado similar)

PAR NEGATIVO (deben estar LEJOS):
‚îú‚îÄ‚îÄ "good morning"
‚îî‚îÄ‚îÄ "that's a noisy trombone"
(significado diferente)
```

### El objetivo del embedding model:

```
Pares positivos ‚Üí vectores CERCANOS
Pares negativos ‚Üí vectores LEJANOS
```

---

## El Proceso de Entrenamiento

### Paso 1: Compilar datos de entrenamiento

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                                                         ‚îÇ
‚îÇ   MILLONES de pares positivos y negativos              ‚îÇ
‚îÇ                                                         ‚îÇ
‚îÇ   Ejemplo:                                             ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ ("dog", "puppy") ‚Üí positivo                      ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ ("dog", "cat") ‚Üí negativo                        ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ ("happy", "joyful") ‚Üí positivo                   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ ("happy", "sad") ‚Üí negativo                      ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ ... millones m√°s                                 ‚îÇ
‚îÇ                                                         ‚îÇ
‚îÇ   Cada palabra/texto aparece en MUCHOS pares           ‚îÇ
‚îÇ   para capturar todas sus relaciones.                  ‚îÇ
‚îÇ                                                         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

### Paso 2: Inicializaci√≥n aleatoria

```
ANTES del entrenamiento:

Cada texto ‚Üí vector ALEATORIO

"dog" ‚Üí [0.23, 0.87, -0.12, ...]  (sin sentido)
"cat" ‚Üí [0.91, -0.34, 0.56, ...]  (sin sentido)
"puppy" ‚Üí [-0.45, 0.12, 0.78, ...] (sin sentido)

Estos vectores NO tienen ninguna relaci√≥n con el significado.
Si usaras este modelo, los resultados ser√≠an BASURA.
```

---

### Paso 3: Contrastive Training

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                                                         ‚îÇ
‚îÇ   El modelo pregunta:                                  ‚îÇ
‚îÇ   "¬øQu√© tan bien puse los pares positivos juntos      ‚îÇ
‚îÇ    y los negativos separados?"                         ‚îÇ
‚îÇ                                                         ‚îÇ
‚îÇ   Usa el CONTRASTE entre positivos y negativos         ‚îÇ
‚îÇ   para evaluar su performance.                         ‚îÇ
‚îÇ                                                         ‚îÇ
‚îÇ   Por eso se llama: CONTRASTIVE TRAINING               ‚îÇ
‚îÇ                                                         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

### Paso 4: Actualizar par√°metros

```
Bas√°ndose en qu√© tan bien lo hizo:

‚îú‚îÄ‚îÄ ACERCAR pares positivos
‚îî‚îÄ‚îÄ ALEJAR pares negativos

Luego REPETIR:
1. Generar nuevos vectores con par√°metros actualizados
2. Evaluar performance con pares positivos/negativos
3. Actualizar par√°metros
4. Repetir muchas veces
```

---

## Ejemplo Visual

### Anchor Point: "he could smell the roses"

```
Par positivo: "a field of fragrant flowers"
Par negativo: "a lion roared majestically"
```

### Inicio del entrenamiento (aleatorio):

```
         ‚Ä¢ "lion roared"
    
    
              ‚Ä¢ "smell roses" (anchor)
    
    
  ‚Ä¢ "fragrant flowers"
  
(posiciones aleatorias, sin sentido)
```

### Durante el entrenamiento:

```
Desde la perspectiva del anchor:

‚îú‚îÄ‚îÄ TIRAR el par positivo m√°s CERCA
‚îî‚îÄ‚îÄ EMPUJAR el par negativo m√°s LEJOS
```

### Despu√©s de muchas rondas:

```
  ‚Ä¢ "smell roses" (anchor)
  ‚Ä¢ "fragrant flowers"
  (muy cerca - par positivo)
  
  
  
  
  
                              ‚Ä¢ "lion roared"
                              (muy lejos - par negativo)
```

---

## La Complejidad Real

### Con millones de pares:

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                                                         ‚îÇ
‚îÇ   Cada vector est√° siendo EMPUJADO y TIRADO            ‚îÇ
‚îÇ   en MUCHAS direcciones simult√°neamente.               ‚îÇ
‚îÇ                                                         ‚îÇ
‚îÇ   "dog" tiene pares con:                               ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ "puppy" (positivo) ‚Üí tirar cerca                 ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ "cat" (negativo) ‚Üí empujar lejos                 ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ "pet" (positivo) ‚Üí tirar cerca                   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ "car" (negativo) ‚Üí empujar lejos                 ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ ... miles m√°s                                    ‚îÇ
‚îÇ                                                         ‚îÇ
‚îÇ   Por eso necesitamos CIENTOS de dimensiones:          ‚îÇ
‚îÇ   Da espacio para acomodar todas las relaciones.       ‚îÇ
‚îÇ                                                         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## Key Takeaways

### 1. Los vectores son abstractos y algo aleatorios

```
ANTES del training:
‚îú‚îÄ‚îÄ Una ubicaci√≥n en el espacio NO tiene significado
‚îî‚îÄ‚îÄ Los vectores se colocan aleatoriamente

DESPU√âS del training:
‚îú‚îÄ‚îÄ Las ubicaciones S√ç tienen significado sem√°ntico
‚îî‚îÄ‚îÄ PERO solo porque ah√≠ se form√≥ un cluster de conceptos similares
```

### 2. Los clusters se forman, pero en ubicaciones diferentes

```
Si entren√°s el mismo modelo dos veces con diferentes valores iniciales:

Run 1: Cluster de "animales" cerca de [0.5, 0.3, ...]
Run 2: Cluster de "animales" cerca de [-0.2, 0.8, ...]

Los MISMOS clusters se forman, pero en DIFERENTES ubicaciones.
```

### 3. NUNCA compares vectores de diferentes modelos

```
‚ùå INCORRECTO:
vector_modelo_A = embed_A("perro")
vector_modelo_B = embed_B("dog")
distance(vector_modelo_A, vector_modelo_B)  # BASURA

Cada modelo tiene:
‚îú‚îÄ‚îÄ Diferentes datos de entrenamiento
‚îú‚îÄ‚îÄ Diferentes dimensiones
‚îú‚îÄ‚îÄ Diferentes valores iniciales aleatorios

Comparar vectores de modelos diferentes = NONSENSE
```

---

## En la Pr√°ctica

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                                                         ‚îÇ
‚îÇ   Probablemente usar√°s modelos PRE-ENTRENADOS          ‚îÇ
‚îÇ   (off-the-shelf)                                      ‚îÇ
‚îÇ                                                         ‚îÇ
‚îÇ   Ejemplos:                                            ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ OpenAI text-embedding-ada-002                    ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ Sentence Transformers (all-MiniLM)               ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ Cohere embed-v3                                  ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ BGE, E5, etc.                                    ‚îÇ
‚îÇ                                                         ‚îÇ
‚îÇ   Hacen un trabajo EXCELENTE ubicando textos           ‚îÇ
‚îÇ   similares en ubicaciones cercanas.                   ‚îÇ
‚îÇ                                                         ‚îÇ
‚îÇ   Probablemente tampoco implementes las m√©tricas       ‚îÇ
‚îÇ   de distancia - las librer√≠as lo hacen por vos.       ‚îÇ
‚îÇ                                                         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## Aplicaci√≥n para DONA üéØ

### Consideraciones para elegir embedding model:

```
1. IDIOMA:
   ‚îú‚îÄ‚îÄ ¬øEl modelo entiende espa√±ol?
   ‚îú‚îÄ‚îÄ ¬øEntiende jerga argentina de construcci√≥n?
   ‚îî‚îÄ‚îÄ Modelos multiling√ºes vs espa√±ol-espec√≠ficos

2. DOMINIO:
   ‚îú‚îÄ‚îÄ ¬øFue entrenado con textos de construcci√≥n?
   ‚îú‚îÄ‚îÄ ¬øEntiende "hierro del 8" vs "varilla 8mm"?
   ‚îî‚îÄ‚îÄ Puede necesitar fine-tuning

3. CONSISTENCIA:
   ‚îú‚îÄ‚îÄ SIEMPRE usar el MISMO modelo para docs y queries
   ‚îú‚îÄ‚îÄ Si cambi√°s de modelo, re-embeber TODO
   ‚îî‚îÄ‚îÄ Nunca mezclar vectores de diferentes modelos
```

### Modelos recomendados para espa√±ol:

```
‚îú‚îÄ‚îÄ multilingual-e5-large (muy bueno en espa√±ol)
‚îú‚îÄ‚îÄ paraphrase-multilingual-MiniLM
‚îú‚îÄ‚îÄ BETO embeddings (espa√±ol espec√≠fico)
‚îî‚îÄ‚îÄ OpenAI ada-002 (multiling√ºe)
```

---

## Resumen del Cap√≠tulo 16

| Concepto | Explicaci√≥n |
|----------|-------------|
| **Contrastive Training** | Entrenar con pares positivos y negativos |
| **Pares positivos** | Textos similares ‚Üí deben estar cerca |
| **Pares negativos** | Textos diferentes ‚Üí deben estar lejos |
| **Inicializaci√≥n** | Vectores aleatorios al inicio |
| **Despu√©s de training** | Clusters de significado se forman |
| **Regla cr√≠tica** | NUNCA comparar vectores de diferentes modelos |

---

## Key Takeaway:

> "Despu√©s del entrenamiento, los vectores capturan significado porque textos similares fueron TIRADOS hacia √°reas similares del espacio vectorial."

---

## Pr√≥ximo: Usando Dense Vectors en el Retriever

C√≥mo usar estos vectores en tu sistema RAG.

---
