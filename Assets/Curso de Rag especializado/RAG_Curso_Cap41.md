# CapÃ­tulo 41: ImplementaciÃ³n de Observabilidad

---

## De MÃ©tricas a ImplementaciÃ³n

> "Una vez que sabÃ©s quÃ© mÃ©tricas querÃ©s colectar sobre tu sistema RAG, realmente necesitÃ¡s construir el sistema para colectar esos datos."

---

## Plataformas de Observabilidad para LLMs

### Por quÃ© usar una plataforma:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                         â”‚
â”‚   Plataformas diseÃ±adas para aplicaciones LLM:         â”‚
â”‚                                                         â”‚
â”‚   â”œâ”€â”€ Capturar mÃ©tricas system-wide y component-level â”‚
â”‚   â”œâ”€â”€ Loggear trÃ¡fico del sistema                      â”‚
â”‚   â”œâ”€â”€ Habilitar experimentaciÃ³n                        â”‚
â”‚                                                         â”‚
â”‚   BENEFICIO:                                           â”‚
â”‚   Menos tiempo diseÃ±ando/implementando observabilidad  â”‚
â”‚   MÃ¡s tiempo monitoreando y mejorando tu sistema       â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Plataformas disponibles:

```
â”œâ”€â”€ Phoenix (Arize) - Open source
â”œâ”€â”€ LangSmith (LangChain)
â”œâ”€â”€ Weights & Biases
â”œâ”€â”€ Helicone
â”œâ”€â”€ Langfuse
â””â”€â”€ Y mÃ¡s...
```

---

## Phoenix (Arize) - Ejemplo

### Herramienta principal: TRACES

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                         â”‚
â”‚   Un TRACE te permite seguir el path de un prompt      â”‚
â”‚   a travÃ©s de TODO el pipeline RAG.                    â”‚
â”‚                                                         â”‚
â”‚   Ver cÃ³mo es modificado por cada componente.          â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### QuÃ© muestra un trace:

```
TRACE DE UN PROMPT:

1. INPUT
   â””â”€â”€ "Â¿CuÃ¡nto sale el cemento Portland?"

2. QUERY REWRITING
   â””â”€â”€ "precio cemento portland"
   â””â”€â”€ Latency: 45ms

3. RETRIEVER
   â””â”€â”€ Query enviada: "precio cemento portland"
   â””â”€â”€ Chunks retornados: 5
   â””â”€â”€ Latency: 120ms

4. RE-RANKER
   â””â”€â”€ Input: 5 chunks
   â””â”€â”€ Output: 3 chunks (top relevantes)
   â””â”€â”€ Latency: 80ms

5. LLM PROMPT
   â””â”€â”€ System prompt + context + query
   â””â”€â”€ Total tokens: 1,247

6. LLM RESPONSE
   â””â”€â”€ "El cemento Portland estÃ¡ a $8,500..."
   â””â”€â”€ Tokens generados: 87
   â””â”€â”€ Latency: 1,200ms

TOTAL LATENCY: 1,445ms
```

### Uso de traces:

```
Para debugging:
â”œâ”€â”€ Prompt tuvo mala performance
â”œâ”€â”€ Seguir el trace
â”œâ”€â”€ Identificar quÃ© paso causÃ³ el error
â”‚
â”‚   Â¿El retriever trajo docs irrelevantes?
â”‚   Â¿El re-ranker eliminÃ³ el doc correcto?
â”‚   Â¿El LLM ignorÃ³ el contexto?
â”‚
â””â”€â”€ Arreglar el componente especÃ­fico
```

---

## IntegraciÃ³n con RAGAS

### Evaluaciones automÃ¡ticas:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                         â”‚
â”‚   Phoenix integra con RAGAS para calcular:             â”‚
â”‚                                                         â”‚
â”‚   â”œâ”€â”€ Search relevancy del retriever                   â”‚
â”‚   â”œâ”€â”€ Faithfulness del LLM                            â”‚
â”‚   â”œâ”€â”€ Citation accuracy                               â”‚
â”‚   â””â”€â”€ Otras mÃ©tricas RAGAS                            â”‚
â”‚                                                         â”‚
â”‚   FÃ¡cil agregar estos pasos de evaluaciÃ³n.             â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ExperimentaciÃ³n

### IteraciÃ³n manual:

```
Probar tus propios prompts y ver cÃ³mo serÃ­an
procesados por tu pipeline RAG.

Ãštil para:
â”œâ”€â”€ Testear edge cases
â”œâ”€â”€ Verificar comportamiento esperado
â””â”€â”€ Debugging durante desarrollo
```

### A/B Testing:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                         â”‚
â”‚   Testear CAMBIOS al sistema:                          â”‚
â”‚                                                         â”‚
â”‚   PREGUNTAS QUE RESPONDE:                              â”‚
â”‚   â”œâ”€â”€ Â¿El nuevo system prompt mejora calidad?          â”‚
â”‚   â”œâ”€â”€ Â¿QuÃ© gains da agregar un re-ranker?             â”‚
â”‚   â”œâ”€â”€ Â¿El nuevo modelo es mejor?                       â”‚
â”‚   â””â”€â”€ Â¿El cambio de alpha mejora retrieval?           â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Reportes Agregados

### EstadÃ­sticas diarias:

```
Phoenix provee reportes de mÃ©tricas clave:

â”œâ”€â”€ Accuracy del retriever
â”œâ”€â”€ Hallucination rate del modelo
â”œâ”€â”€ Latencia promedio
â”œâ”€â”€ Error rates
â”œâ”€â”€ Throughput
â””â”€â”€ Tendencias over time
```

### Dashboard tÃ­pico:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     DONA - Daily Report                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                         â”‚
â”‚   ðŸ“Š Retriever Accuracy: 87% (â†‘2% vs ayer)            â”‚
â”‚   ðŸŽ¯ Response Relevancy: 0.89 (estable)                â”‚
â”‚   âš ï¸ Hallucination Rate: 3.2% (â†“0.5% vs ayer)         â”‚
â”‚   â±ï¸ Avg Latency: 1.8s (estable)                       â”‚
â”‚   ðŸ“ˆ Requests: 1,247 (â†‘15% vs ayer)                    â”‚
â”‚   ðŸ‘ User Satisfaction: 91%                            â”‚
â”‚                                                         â”‚
â”‚   [Ver detalles] [Exportar] [Comparar perÃ­odos]        â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Herramientas Complementarias

### Lo que Phoenix NO cubre bien:

```
â”œâ”€â”€ Compute usage de la vector database
â”œâ”€â”€ Memory usage del sistema
â”œâ”€â”€ Infraestructura general
â””â”€â”€ Networking metrics
```

### Herramientas clÃ¡sicas para eso:

```
â”œâ”€â”€ Datadog
â”œâ”€â”€ Grafana
â”œâ”€â”€ Prometheus
â”œâ”€â”€ CloudWatch (AWS)
â”œâ”€â”€ Cloud Monitoring (GCP)
â””â”€â”€ Application Insights (Azure)
```

### Setup completo:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                         â”‚
â”‚   OBSERVABILIDAD COMPLETA:                             â”‚
â”‚                                                         â”‚
â”‚   LLM-ESPECÃFICO (Phoenix/LangSmith):                  â”‚
â”‚   â”œâ”€â”€ Traces de prompts                                â”‚
â”‚   â”œâ”€â”€ MÃ©tricas de calidad RAG                         â”‚
â”‚   â”œâ”€â”€ ExperimentaciÃ³n                                  â”‚
â”‚   â””â”€â”€ Evals con RAGAS                                 â”‚
â”‚                                                         â”‚
â”‚   INFRAESTRUCTURA (Datadog/Grafana):                   â”‚
â”‚   â”œâ”€â”€ CPU/Memory/Disk usage                           â”‚
â”‚   â”œâ”€â”€ Network metrics                                  â”‚
â”‚   â”œâ”€â”€ Database performance                             â”‚
â”‚   â””â”€â”€ Error logs                                       â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## El Flywheel de Mejoras

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                         â”‚
â”‚          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”‚
â”‚          â”‚ Ver cÃ³mo el sistema  â”‚                      â”‚
â”‚          â”‚ maneja trÃ¡fico real  â”‚                      â”‚
â”‚          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚
â”‚                     â”‚                                   â”‚
â”‚                     â–¼                                   â”‚
â”‚          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”‚
â”‚          â”‚ Identificar bugs o   â”‚                      â”‚
â”‚          â”‚ Ã¡reas de mejora      â”‚                      â”‚
â”‚          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚
â”‚                     â”‚                                   â”‚
â”‚                     â–¼                                   â”‚
â”‚          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”‚
â”‚          â”‚ Hacer cambios        â”‚                      â”‚
â”‚          â”‚ (experimentos)       â”‚                      â”‚
â”‚          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚
â”‚                     â”‚                                   â”‚
â”‚                     â–¼                                   â”‚
â”‚          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”‚
â”‚          â”‚ Ver impacto de los   â”‚â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚          â”‚ cambios              â”‚      â”‚               â”‚
â”‚          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚               â”‚
â”‚                     â–²                   â”‚               â”‚
â”‚                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚                                                         â”‚
â”‚   Con el tiempo: tunear cada componente para           â”‚
â”‚   matchear cÃ³mo los usuarios REALMENTE usan el sistema.â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Custom Datasets

### La idea:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                         â”‚
â”‚   Crear DATASETS CUSTOM de prompts que tu sistema      â”‚
â”‚   RAG ya procesÃ³ anteriormente.                        â”‚
â”‚                                                         â”‚
â”‚   GUARDAR prompts reales â†’ RE-CORRER por el sistema    â”‚
â”‚   â†’ Ver IMPACTO de cambios en prompts reales           â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

Esto se explora mÃ¡s en el prÃ³ximo video.

---

## AplicaciÃ³n para DONA ðŸŽ¯

### Setup de observabilidad para DONA:

```python
# Phoenix setup para DONA
import phoenix as px
from phoenix.trace.openai import OpenAIInstrumentor

# Instrumentar calls
OpenAIInstrumentor().instrument()

# Iniciar session
session = px.launch_app()

# Ahora todos los calls se tracean automÃ¡ticamente
```

### Trace tÃ­pico en DONA:

```
TRACE: "che cuanto sale el fierro del 8"

1. INTENT DETECTION (50ms)
   â””â”€â”€ Intent: "consulta_precio"

2. QUERY REWRITING (80ms)
   â””â”€â”€ "precio hierro 8mm varilla construcciÃ³n"

3. HYBRID SEARCH (150ms)
   â””â”€â”€ Retrieved: 8 docs
   â””â”€â”€ Top score: 0.87

4. RE-RANKING (100ms)
   â””â”€â”€ After rerank: 5 docs
   â””â”€â”€ Top score: 0.92

5. LLM GENERATION (1,200ms)
   â””â”€â”€ Model: gpt-3.5-turbo
   â””â”€â”€ Input tokens: 850
   â””â”€â”€ Output tokens: 120

6. RESPONSE
   â””â”€â”€ "El hierro del 8 estÃ¡ a $X el metro..."

TOTAL: 1,580ms
COST: $0.0015
```

### Dashboard DONA:

```python
DONA_DASHBOARD_METRICS = {
    "daily": {
        "requests_total": count,
        "avg_latency_ms": avg,
        "p95_latency_ms": percentile(95),
        "error_rate": rate,
        "thumbs_up_rate": rate,
        "cost_total_usd": sum
    },
    
    "quality": {
        "retriever_accuracy": ragas_metric,
        "response_relevancy": ragas_metric,
        "faithfulness": ragas_metric,
        "price_accuracy": custom_metric  # Â¿Precios correctos?
    },
    
    "alerts": {
        "latency_spike": if p95 > 5000,
        "error_spike": if error_rate > 0.05,
        "quality_drop": if faithfulness < 0.80
    }
}
```

---

## Resumen del CapÃ­tulo 41

| Herramienta | QuÃ© hace | CuÃ¡ndo usar |
|-------------|----------|-------------|
| **Phoenix/LangSmith** | Traces, evals RAG, experimentaciÃ³n | Siempre para LLMs |
| **RAGAS** | MÃ©tricas de calidad | Evaluar retriever/LLM |
| **Datadog/Grafana** | Infra, compute, memory | Monitoreo general |

---

## Key Takeaways:

```
1. Usar PLATAFORMAS de observabilidad para LLMs
   (Phoenix, LangSmith, etc.)

2. TRACES son la herramienta principal para debugging
   (seguir el journey del prompt)

3. Integrar con RAGAS para mÃ©tricas de calidad

4. Complementar con herramientas clÃ¡sicas (Datadog)
   para infraestructura

5. El FLYWHEEL: observar â†’ identificar â†’ cambiar â†’ medir â†’ repetir

6. CUSTOM DATASETS de trÃ¡fico real para testear cambios
```

---

## PrÃ³ximo: Construyendo Custom Datasets

Crear datasets de prompts reales para testing.

---
