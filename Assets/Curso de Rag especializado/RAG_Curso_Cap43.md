# Cap√≠tulo 43: Quantization - Compresi√≥n para LLMs y Vectores

---

## Los Trade-offs de Producci√≥n

> "Una vez que pod√©s evaluar tu sistema RAG y experimentar con diferentes configuraciones, vas a estar listo para enfrentar trade-offs familiares en muchos proyectos de software: Costo, Velocidad, y Calidad."

---

## ¬øQu√© es Quantization?

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                                                         ‚îÇ
‚îÇ   QUANTIZATION = Compresi√≥n para LLMs y Vectores       ‚îÇ
‚îÇ                                                         ‚îÇ
‚îÇ   Reemplaza:                                           ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ Model weights en LLMs                            ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ Valores en embedding vectors                     ‚îÇ
‚îÇ                                                         ‚îÇ
‚îÇ   Con tipos de datos de MENOR PRECISI√ìN (comprimidos). ‚îÇ
‚îÇ                                                         ‚îÇ
‚îÇ   RESULTADO:                                           ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ M√°s peque√±os                                     ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ M√°s baratos                                      ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ M√°s r√°pidos                                      ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ Con poca p√©rdida de calidad                     ‚îÇ
‚îÇ                                                         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## Analog√≠a: Compresi√≥n de Im√°genes

```
IMAGEN ORIGINAL (24 bits/pixel):
‚îî‚îÄ‚îÄ Colores perfectos
‚îî‚îÄ‚îÄ Mucha data

IMAGEN COMPRIMIDA (12 bits/pixel):
‚îî‚îÄ‚îÄ 50% del tama√±o
‚îî‚îÄ‚îÄ Calidad aceptable

IMAGEN MUY COMPRIMIDA (6 bits/pixel):
‚îî‚îÄ‚îÄ 25% del tama√±o
‚îî‚îÄ‚îÄ Artifacts visibles

Dependiendo del uso, la p√©rdida de calidad
puede valer el ahorro de memoria.
```

---

## Quantization de LLMs

### El problema:

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                                                         ‚îÇ
‚îÇ   Par√°metros t√≠picos de LLM: 16 bits cada uno          ‚îÇ
‚îÇ                                                         ‚îÇ
‚îÇ   Modelos modernos:                                    ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ 1 billion ‚Üí 1 trillion par√°metros               ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ = ENORMES                                        ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ Mucha memoria para guardarlos                    ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ GPUs potentes para correrlos                     ‚îÇ
‚îÇ                                                         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### La soluci√≥n:

```
QUANTIZED LLMs:
16-bit ‚Üí 8-bit o 4-bit

BENEFICIOS:
‚îú‚îÄ‚îÄ Reduce GPU memory significativamente
‚îú‚îÄ‚îÄ Modelos corren m√°s r√°pido
‚îî‚îÄ‚îÄ Peque√±a p√©rdida de calidad

EJEMPLO:
Llama 70B en 16-bit: ~140GB VRAM
Llama 70B en 4-bit:  ~35GB VRAM (4x menos!)
```

---

## Quantization de Embedding Vectors

### El problema:

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                                                         ‚îÇ
‚îÇ   Vector t√≠pico de 768 dimensiones:                    ‚îÇ
‚îÇ   768 √ó 32-bit floats = 3KB por vector                ‚îÇ
‚îÇ                                                         ‚îÇ
‚îÇ   Modelos de m√°s dimensiones: muchos m√°s KB           ‚îÇ
‚îÇ                                                         ‚îÇ
‚îÇ   Con MILLONES o BILLONES de vectores:                ‚îÇ
‚îÇ   = CANTIDADES ENORMES de data                        ‚îÇ
‚îÇ                                                         ‚îÇ
‚îÇ   Vectores necesitan estar en RAM para b√∫squeda       ‚îÇ
‚îÇ   r√°pida = CARO                                        ‚îÇ
‚îÇ                                                         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Integer Quantization (8-bit):

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                                                         ‚îÇ
‚îÇ   32-bit float ‚Üí 8-bit integer                         ‚îÇ
‚îÇ                                                         ‚îÇ
‚îÇ   RESULTADO: Vectores son 1/4 del tama√±o original     ‚îÇ
‚îÇ                                                         ‚îÇ
‚îÇ   = AHORRO MASIVO de espacio                          ‚îÇ
‚îÇ                                                         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### C√≥mo funciona:

```
PASO 1: Encontrar min y max de cada dimensi√≥n
        (define el rango de valores)

PASO 2: Dividir el rango en 256 secciones
        (256 = cantidad de valores con 8 bits)

PASO 3: Numerar secciones 0, 1, 2, ... 255

PASO 4: Asignar a cada float original
        el n√∫mero de la secci√≥n donde cae

PASO 5: Guardar min y width de cada secci√≥n
        para poder reconstruir aproximaci√≥n

RESULTADO:
Original: 3.14159 (32 bits)
Quantized: 47 (8 bits) + metadata
```

### Performance de 8-bit:

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                                                         ‚îÇ
‚îÇ   A pesar de usar solo 1/4 de la data                  ‚îÇ
‚îÇ   y un algoritmo de compresi√≥n "naive":                ‚îÇ
‚îÇ                                                         ‚îÇ
‚îÇ   8-bit integer quantization funciona MUY BIEN         ‚îÇ
‚îÇ                                                         ‚îÇ
‚îÇ   Recall@K: Solo baja unos pocos puntos porcentuales   ‚îÇ
‚îÇ                                                         ‚îÇ
‚îÇ   BENEFICIOS:                                          ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ Menos data en vector database                   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ B√∫squeda m√°s r√°pida                              ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ C√°lculos simplificados                          ‚îÇ
‚îÇ                                                         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## Binary Quantization (1-bit)

### Compresi√≥n extrema:

```
32-bit float ‚Üí 1-bit

= Compresi√≥n de 32x (!)

Cada valor en el vector es solo 1 o 0
(indica si el valor original era positivo o negativo)
```

### Trade-offs:

```
‚úÖ VENTAJAS:
‚îú‚îÄ‚îÄ Vectores 32x m√°s peque√±os
‚îú‚îÄ‚îÄ Retrieval significativamente m√°s r√°pido
‚îî‚îÄ‚îÄ Much√≠simo ahorro de memoria

‚ùå DESVENTAJAS:
‚îú‚îÄ‚îÄ Performance puede caer notablemente
‚îî‚îÄ‚îÄ P√©rdida de informaci√≥n significativa
```

### T√©cnica combinada:

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                                                         ‚îÇ
‚îÇ   1. RETRIEVAL INICIAL: Con 1-bit vectors (r√°pido)    ‚îÇ
‚îÇ   2. RESCORING: Con 32-bit originales (preciso)       ‚îÇ
‚îÇ                                                         ‚îÇ
‚îÇ   = Lo mejor de ambos mundos                          ‚îÇ
‚îÇ                                                         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## Matryoshka Embedding Models

### La idea (como mu√±ecas rusas):

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                                                         ‚îÇ
‚îÇ   Vectores dise√±ados para usar SOLO UN SUBCONJUNTO    ‚îÇ
‚îÇ   de las dimensiones.                                  ‚îÇ
‚îÇ                                                         ‚îÇ
‚îÇ   Vector de 1000 dimensiones:                          ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ Pod√©s usar solo las primeras 500                ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ O solo las primeras 100                         ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ O el vector completo                            ‚îÇ
‚îÇ                                                         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Por qu√© funciona:

```
MODELO T√çPICO:
Cada dimensi√≥n tiene ~igual varianza (informaci√≥n)

MODELO MATRYOSHKA:
Dimensiones ORDENADAS por densidad de informaci√≥n

‚îú‚îÄ‚îÄ Dimensiones tempranas: M√ÅS varianza = M√ÅS informaci√≥n
‚îî‚îÄ‚îÄ Dimensiones tard√≠as: MENOS varianza = MENOS informaci√≥n

‚Üí Pod√©s excluir dimensiones tard√≠as con menor penalidad
```

### Usos:

```
OPCI√ìN 1: Siempre usar primeras 100 dims
‚îú‚îÄ‚îÄ Ahorro de espacio
‚îú‚îÄ‚îÄ C√°lculos m√°s r√°pidos
‚îî‚îÄ‚îÄ Preserva m√°xima informaci√≥n posible

OPCI√ìN 2: Retrieval + Rescoring
‚îú‚îÄ‚îÄ Initial retrieval: primeras 100 dims (r√°pido)
‚îú‚îÄ‚îÄ Pull remaining 900 dims de memoria m√°s lenta
‚îî‚îÄ‚îÄ Rescore con todas 1000 dims (preciso)

IDEAL PARA:
Ambientes din√°micos donde quer√©s cambiar r√°pidamente
entre representaciones de baja y alta fidelidad.
```

---

## Resumen de T√©cnicas de Quantization

| T√©cnica | Compresi√≥n | Calidad | Uso |
|---------|------------|---------|-----|
| **8-bit Integer** | 4x | Alta | Uso general, recomendado |
| **4-bit Integer** | 8x | Media-Alta | LLMs, balance |
| **1-bit Binary** | 32x | Media | Initial retrieval + rescore |
| **Matryoshka** | Variable | Variable | Ambientes din√°micos |

---

## El Takeaway Principal

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                                                         ‚îÇ
‚îÇ   DEBER√çAS EXPERIMENTAR con quantization:              ‚îÇ
‚îÇ                                                         ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ LLMs quantizados (8-bit, 4-bit)                 ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ Embedding vectors quantizados                    ‚îÇ
‚îÇ                                                         ‚îÇ
‚îÇ   La mayor√≠a de proveedores ofrecen modelos            ‚îÇ
‚îÇ   quantizados junto a los base.                        ‚îÇ
‚îÇ                                                         ‚îÇ
‚îÇ   AHORROS de espacio y costo: SIGNIFICATIVOS           ‚îÇ
‚îÇ   REDUCCIONES de calidad: PEQUE√ëAS                     ‚îÇ
‚îÇ                                                         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## Aplicaci√≥n para DONA üéØ

### D√≥nde aplicar quantization en DONA:

```python
DONA_QUANTIZATION_OPTIONS = {
    "embedding_model": {
        "full_precision": "768 dims √ó 32-bit = 3KB/vector",
        "8-bit_quantized": "768 dims √ó 8-bit = 768B/vector (4x menos)",
        "recommendation": "Empezar con 8-bit, medir recall"
    },
    
    "llm": {
        "full_precision": "gpt-4 via API (no aplica)",
        "self_hosted": {
            "llama_70b_16bit": "~140GB VRAM",
            "llama_70b_8bit": "~70GB VRAM",
            "llama_70b_4bit": "~35GB VRAM"
        },
        "recommendation": "Si self-hosted, probar 8-bit primero"
    },
    
    "vector_database": {
        "weaviate": "Soporta PQ y BQ",
        "pinecone": "Soporta quantization",
        "recommendation": "Habilitar si >1M vectores"
    }
}
```

### C√°lculo de ahorro para DONA:

```
ESCENARIO: Cat√°logo de 100,000 productos

SIN QUANTIZATION:
100,000 √ó 3KB = 300MB de vectores
(necesitan estar en RAM para b√∫squeda r√°pida)

CON 8-BIT QUANTIZATION:
100,000 √ó 768B = 75MB de vectores
= 75% AHORRO de memoria

CON BINARY (1-bit):
100,000 √ó 96B = 9.6MB de vectores
= 97% AHORRO (pero menor recall)
```

### Estrategia recomendada para DONA:

```
FASE 1 (MVP):
‚îú‚îÄ‚îÄ Full precision embeddings
‚îú‚îÄ‚îÄ API-based LLM (no necesita quantization)
‚îî‚îÄ‚îÄ Evaluar baseline de calidad

FASE 2 (Optimizaci√≥n):
‚îú‚îÄ‚îÄ Probar 8-bit quantized embeddings
‚îú‚îÄ‚îÄ Medir impacto en recall
‚îú‚îÄ‚îÄ Si recall baja <3%, adoptar

FASE 3 (Escala):
‚îú‚îÄ‚îÄ Si >1M productos, considerar binary + rescoring
‚îú‚îÄ‚îÄ Si self-hosting LLM, usar 4-bit o 8-bit
‚îî‚îÄ‚îÄ Matryoshka si necesit√°s flexibilidad
```

---

## Resumen del Cap√≠tulo 43

| Concepto | Qu√© hace | Ahorro | P√©rdida |
|----------|----------|--------|---------|
| **LLM Quantization** | 16-bit ‚Üí 8/4-bit | 2-4x memoria | Peque√±a |
| **Vector 8-bit** | 32-bit ‚Üí 8-bit | 4x memoria | ~2-3% recall |
| **Vector Binary** | 32-bit ‚Üí 1-bit | 32x memoria | Notable |
| **Matryoshka** | Usar subset dims | Variable | Variable |

---

## Key Takeaways:

```
1. Quantization = compresi√≥n para LLMs y vectores

2. 8-bit quantization: 4x ahorro con m√≠nima p√©rdida

3. Binary (1-bit): 32x ahorro pero combinar con rescoring

4. Matryoshka: dimensiones ordenadas por informaci√≥n,
   usar solo las que necesit√°s

5. EXPERIMENT√Å con modelos quantizados - los ahorros
   son significativos y las p√©rdidas son peque√±as
```

---

## Pr√≥ximo: M√°s Trade-offs de Producci√≥n

Otras estrategias para balancear costo, velocidad, y calidad.

---
