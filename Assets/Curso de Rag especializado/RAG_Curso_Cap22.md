# CapÃ­tulo 22: TÃ©cnicas Avanzadas de Chunking

---

## El Problema del Chunking BÃ¡sico

> "Al dividir documentos en chunks mÃ¡s pequeÃ±os, tambiÃ©n se arriesga a romper el texto de manera que pierde contexto relevante."

### Ejemplo:

```
OraciÃ³n original:
"That night she dreamed, as she did often, 
 that she was finally an Olympic champion."

Si el chunk corta aquÃ­:
"That night she dreamed... | ...that she was finally an Olympic champion."

Parece que YA ES campeona olÃ­mpica,
cuando en realidad estÃ¡ SOÃ‘ANDO con serlo.
```

> "Fixed size y recursive character splitting no proveen protecciÃ³n contra este tipo de problema."

---

## TÃ©cnica 1: Semantic Chunking

### La idea:

```
Agrupar oraciones en chunks si tienen SIGNIFICADO SIMILAR.
```

### El algoritmo:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                         â”‚
â”‚   1. Empezar con la primera oraciÃ³n                    â”‚
â”‚                                                         â”‚
â”‚   2. Para cada oraciÃ³n siguiente:                      â”‚
â”‚      a) Vectorizar el chunk actual                     â”‚
â”‚      b) Vectorizar la oraciÃ³n siguiente                â”‚
â”‚      c) Calcular distancia entre vectores              â”‚
â”‚                                                         â”‚
â”‚   3. Si distancia < threshold:                         â”‚
â”‚      â†’ Agregar oraciÃ³n al chunk actual                 â”‚
â”‚                                                         â”‚
â”‚   4. Si distancia > threshold:                         â”‚
â”‚      â†’ Cortar chunk aquÃ­                               â”‚
â”‚      â†’ Empezar nuevo chunk con esta oraciÃ³n            â”‚
â”‚                                                         â”‚
â”‚   5. Repetir hasta terminar el documento               â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### VisualizaciÃ³n:

```
Distancia entre chunk actual y siguiente oraciÃ³n:

Threshold â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                    â”‚              â”‚
                    â”‚              â”‚    â•±â•²
               â•±â•²   â”‚         â•±â•²   â”‚   â•±  â•²
          â•±â•²  â•±  â•²  â”‚    â•±â•²  â•±  â•²  â”‚  â•±    
     â•±â•²  â•±  â•²â•±    â•² â”‚   â•±  â•²â•±    â•² â”‚ â•±     
â”€â”€â”€â”€â•±â”€â”€â•²â•±â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•²â”‚â”€â”€â•±â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•²â”‚â•±â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                    â”‚              â”‚
                 CORTE          CORTE
                   â†“              â†“
              Chunk 1        Chunk 2        Chunk 3
```

### Resultado:

```
âœ… Chunks de tamaÃ±o variable
âœ… Siguen el "hilo de pensamiento" del autor
âœ… Si el autor hace una tangente â†’ nuevo chunk
âœ… Si el mismo tema cruza pÃ¡rrafos â†’ mismo chunk
```

### Trade-off:

```
âŒ Computacionalmente COSTOSO
   â””â”€â”€ Vectorizar CADA oraciÃ³n de la knowledge base

âœ… Alta calidad de retrieval
   â””â”€â”€ Mejor precision y recall
```

---

## TÃ©cnica 2: LLM-Based Chunking

### La idea:

```
Darle el documento a un LLM con instrucciones
de cÃ³mo querÃ©s que cree los chunks.
```

### Ejemplo de prompt:

```
"SeparÃ¡ este documento en chunks basÃ¡ndote en el significado.
MantenÃ© conceptos similares juntos en un chunk.
SeparÃ¡ el texto en nuevos chunks cuando se discutan nuevos temas."
```

### CaracterÃ­sticas:

```
âœ… MUY alta performance
âœ… Flexible - podÃ©s dar instrucciones especÃ­ficas
âœ… Costos de LLM bajando â†’ mÃ¡s viable econÃ³micamente

âŒ "Black box" - difÃ­cil auditar
âŒ Costoso en procesamiento
âŒ Resultados pueden variar
```

---

## TÃ©cnica 3: Context-Aware Chunking

### La idea:

```
Usar un LLM para AGREGAR CONTEXTO a cada chunk,
explicando su rol en el documento mÃ¡s amplio.
```

### Ejemplo:

```
DOCUMENTO: Blog post sobre entrenamiento para maratÃ³n

CHUNK ORIGINAL (final del blog):
"Gracias a Juan PÃ©rez, MarÃ­a GarcÃ­a, 
 Pedro LÃ³pez, y todo el equipo de Nike."

PROBLEMA: Solo una lista de nombres, difÃ­cil de interpretar.

CHUNK CON CONTEXTO AGREGADO:
"[Este chunk contiene los agradecimientos finales 
del blog post sobre preparaciÃ³n para maratÃ³n. 
El autor agradece a sus entrenadores y sponsors.]

Gracias a Juan PÃ©rez, MarÃ­a GarcÃ­a, 
Pedro LÃ³pez, y todo el equipo de Nike."
```

### Beneficios del contexto agregado:

```
1. CUANDO SE VECTORIZA:
   â””â”€â”€ El contexto mejora el embedding
   â””â”€â”€ â†’ Mejor search relevancy

2. CUANDO SE RECUPERA:
   â””â”€â”€ El LLM entiende mejor el chunk
   â””â”€â”€ â†’ Mejor generaciÃ³n de respuesta
```

### Trade-off:

```
âŒ Preprocessing costoso
   â””â”€â”€ LLM debe procesar CADA documento y chunk

âœ… BÃºsquedas mÃ¡s relevantes
âœ… Sin impacto en velocidad de bÃºsqueda
   â””â”€â”€ El costo es una sola vez, no por query
```

---

## ComparaciÃ³n de TÃ©cnicas

| TÃ©cnica | Complejidad | Costo | Calidad | CuÃ¡ndo usar |
|---------|-------------|-------|---------|-------------|
| **Fixed size + overlap** | Baja | Bajo | OK | Prototipos, default |
| **Recursive character** | Baja | Bajo | OK+ | Docs estructurados |
| **Semantic chunking** | Media | Alto | Alta | Cuando precision importa |
| **LLM-based** | Alta | Muy alto | Muy alta | Cuando presupuesto permite |
| **Context-aware** | Media | Alto | Alta+ | Primer upgrade a explorar |

---

## Recomendaciones de Uso

### Para empezar:

```
1. Fixed-size con overlap (500 chars, 10% overlap)
   â””â”€â”€ Bueno para prototipos
   â””â”€â”€ Baseline decente

2. Si necesitÃ¡s mejorar:
   â””â”€â”€ Context-aware chunking es buen primer paso
   â””â”€â”€ Aplica sobre cualquier estrategia
   â””â”€â”€ Mejora tanto search como generation
```

### Para producciÃ³n:

```
1. Experimentar con subset pequeÃ±o de datos
2. Medir si tÃ©cnicas avanzadas REALMENTE mejoran
3. Evaluar costo vs beneficio

NO es el objetivo usar la tÃ©cnica mÃ¡s cutting-edge.
ES el objetivo entender quÃ© opciones hay y cuÃ¡les
son apropiadas para TUS datos y TU sistema.
```

---

## AplicaciÃ³n para DONA ğŸ¯

### AnÃ¡lisis por tipo de documento:

```
FICHAS DE PRODUCTO (cortas):
â”œâ”€â”€ Probablemente fixed-size es suficiente
â”œâ”€â”€ Ya son semÃ¡nticamente coherentes
â””â”€â”€ Context-aware no agrega mucho valor

MANUALES TÃ‰CNICOS (largos):
â”œâ”€â”€ Semantic chunking puede ayudar
â”œâ”€â”€ Secciones tÃ©cnicas tienen temas claros
â””â”€â”€ Context-aware Ãºtil para secciones ambiguas

CATÃLOGO GENERAL:
â”œâ”€â”€ Fixed-size por producto
â”œâ”€â”€ Metadata es mÃ¡s importante que chunking sofisticado
â””â”€â”€ Filtrar por categoria > chunking avanzado
```

### Estrategia recomendada para DONA:

```
FASE 1 (MVP):
â”œâ”€â”€ Fixed-size + overlap para todo
â”œâ”€â”€ Metadata bien configurada
â””â”€â”€ Evaluar mÃ©tricas baseline

FASE 2 (Si es necesario):
â”œâ”€â”€ Context-aware para manuales tÃ©cnicos
â”œâ”€â”€ Agregar resumen de secciÃ³n a cada chunk
â””â”€â”€ Medir si mejora precision/recall

FASE 3 (Si justifica el costo):
â”œâ”€â”€ Semantic chunking para documentaciÃ³n compleja
â”œâ”€â”€ Solo si mÃ©tricas muestran problemas
â””â”€â”€ Evaluar ROI del costo computacional
```

### Ejemplo de context-aware para manual:

```python
# Chunk original de manual de cemento
chunk = "Mezclar 3 partes de arena por 1 de cemento..."

# Con contexto agregado por LLM
chunk_con_contexto = """
[Contexto: Esta secciÃ³n del Manual de Cemento Portland 
describe la proporciÃ³n de mezcla para mortero de 
albaÃ±ilerÃ­a bÃ¡sico. Es parte del capÃ­tulo de 
"Aplicaciones Residenciales".]

Mezclar 3 partes de arena por 1 de cemento...
"""
```

---

## Resumen del CapÃ­tulo 22

| TÃ©cnica | QuÃ© hace | Costo | CuÃ¡ndo usar |
|---------|----------|-------|-------------|
| **Semantic** | Agrupa por significado similar | Alto | Precision crÃ­tica |
| **LLM-based** | LLM decide dÃ³nde cortar | Muy alto | Presupuesto alto |
| **Context-aware** | Agrega contexto a chunks | Alto | Primer upgrade |

---

## Key Takeaway:

> "El objetivo NO es implementar la tÃ©cnica de chunking mÃ¡s cutting-edge del mercado. Es entender quÃ© opciones estÃ¡n disponibles, quÃ© tan adecuadas son para tus datos, y si los costos y beneficios hacen que valga la pena implementarlas en tu sistema."

---

## PrÃ³ximo: Query Parsing

CÃ³mo procesar y entender las queries de los usuarios.

---
