# Cap√≠tulo 30: Eligiendo el LLM Correcto

---

## La Decisi√≥n Importante

> "Una decisi√≥n mayor cuando constru√≠s una aplicaci√≥n RAG es qu√© LLM vas a usar. Elegir el correcto puede tener un gran impacto en velocidad, calidad y presupuesto."

---

## M√©tricas Cuantificables

### 1. Model Size (Par√°metros)

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                                                         ‚îÇ
‚îÇ   PEQUE√ëOS: 1-10 billion par√°metros                    ‚îÇ
‚îÇ   MEDIANOS: 10-100 billion par√°metros                  ‚îÇ
‚îÇ   GRANDES: 100-500+ billion par√°metros                 ‚îÇ
‚îÇ                                                         ‚îÇ
‚îÇ   M√°s grande = t√≠picamente m√°s capaz                   ‚îÇ
‚îÇ   M√°s grande = SIEMPRE m√°s caro de correr              ‚îÇ
‚îÇ                                                         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### 2. Costo

```
T√≠picamente cobrado por mill√≥n de tokens.

A veces precios diferentes para:
‚îú‚îÄ‚îÄ Input tokens (prompt)
‚îî‚îÄ‚îÄ Output tokens (completion)

Modelos m√°s nuevos/grandes/capaces = m√°s caros.
```

### 3. Context Window

```
M√°ximo de tokens que el LLM puede procesar
(prompt + completion combinados).

Ejemplos:
‚îú‚îÄ‚îÄ GPT-3.5: ~16K tokens
‚îú‚îÄ‚îÄ GPT-4: ~128K tokens
‚îú‚îÄ‚îÄ Claude: ~200K tokens
‚îú‚îÄ‚îÄ Gemini: ~1M tokens

M√°s grande = m√°s flexibilidad
PERO: Segu√≠s pagando por token usado.
```

### 4. Velocidad / Latencia

```
TIME TO FIRST TOKEN:
¬øCu√°nto tarda en empezar a responder?

TOKENS PER SECOND:
¬øQu√© tan r√°pido genera una vez que empieza?

Si tu sistema necesita real-time:
‚Üí Podr√≠as sacrificar calidad por velocidad.
```

### 5. Knowledge Cutoff Date

```
¬øHasta qu√© fecha tiene conocimiento el modelo?

Incluso en RAG, un cutoff m√°s reciente es preferible:
‚îú‚îÄ‚îÄ Mejor comprensi√≥n de eventos recientes
‚îú‚îÄ‚îÄ Conocimiento m√°s actualizado del mundo
‚îî‚îÄ‚îÄ Menos chances de informaci√≥n obsoleta
```

---

## M√©tricas de Calidad (M√°s Dif√≠ciles)

### El desaf√≠o:

```
"Calidad" incluye:
‚îú‚îÄ‚îÄ Capacidad de razonamiento
‚îú‚îÄ‚îÄ Resoluci√≥n de problemas matem√°ticos
‚îú‚îÄ‚îÄ Generaci√≥n de c√≥digo
‚îú‚îÄ‚îÄ Texto agradable de leer
‚îú‚îÄ‚îÄ Seguimiento de instrucciones
‚îî‚îÄ‚îÄ Y mucho m√°s...

No hay una sola m√©trica que capture todo.
```

---

## Tipos de Benchmarks

### 1. Automated Benchmarks

```
LLMs evaluados en tareas que c√≥digo puede verificar.

Ejemplos:
‚îú‚îÄ‚îÄ Multiple choice tests
‚îú‚îÄ‚îÄ Problemas matem√°ticos
‚îú‚îÄ‚îÄ Coding challenges

MMLU (Massive Multitask Language Understanding):
‚îú‚îÄ‚îÄ 57 subjects
‚îú‚îÄ‚îÄ STEM, humanities, law, etc.
‚îú‚îÄ‚îÄ Multiple choice format
```

### 2. Human Evaluation

```
Dos LLMs an√≥nimos responden al mismo prompt.
Humanos eligen cu√°l respuesta prefieren.

Se usa algoritmo ELO (como en ajedrez)
para crear un ranking comparativo.

LLM Arena:
‚îú‚îÄ‚îÄ Uno de los benchmarks m√°s citados
‚îú‚îÄ‚îÄ Captura calidad matizada
‚îú‚îÄ‚îÄ Factores que automated no puede medir
```

### 3. LLM-as-a-Judge

```
Un LLM eval√∫a las respuestas de otro LLM.

El juez tiene respuestas de referencia
y determina qu√© tan cerca estuvo el evaluado.

Resultado: Win rate para comparar modelos.

‚úÖ Ventaja: Barato y flexible
‚ùå Desventaja: Bias hacia su propia familia
   GPT prefiere GPT, Gemini prefiere Gemini
```

---

## Cualidades de Buenos Benchmarks

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                                                         ‚îÇ
‚îÇ   1. RELEVANCIA                                        ‚îÇ
‚îÇ      Si tu app no genera c√≥digo,                       ‚îÇ
‚îÇ      un benchmark de c√≥digo no ayuda.                  ‚îÇ
‚îÇ                                                         ‚îÇ
‚îÇ   2. DIFICULTAD                                        ‚îÇ
‚îÇ      Debe diferenciar entre modelos.                   ‚îÇ
‚îÇ      Si todos scorean bien, no es √∫til.                ‚îÇ
‚îÇ                                                         ‚îÇ
‚îÇ   3. REPRODUCIBILIDAD                                  ‚îÇ
‚îÇ      Scores consistentes entre runs.                   ‚îÇ
‚îÇ      Verificables independientemente.                  ‚îÇ
‚îÇ                                                         ‚îÇ
‚îÇ   4. ALINEAMIENTO CON REALIDAD                         ‚îÇ
‚îÇ      Buen score en coding ‚Üí debe escribir buen c√≥digo  ‚îÇ
‚îÇ      en la pr√°ctica, no solo en el benchmark.          ‚îÇ
‚îÇ                                                         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## El Problema de Data Contamination

```
LLMs se entrenan con trillones de tokens del internet.

Posible que datos del benchmark est√©n en el training data.

Si el LLM YA VIO las preguntas y respuestas:
‚îú‚îÄ‚îÄ Overperforma en ese benchmark
‚îú‚îÄ‚îÄ Pero no refleja capacidad real
‚îî‚îÄ‚îÄ Score no es indicativo de rendimiento general
```

---

## La Realidad de los Benchmarks

### El patr√≥n que se repite:

```
A√ëO 1: Benchmark nuevo, scores bajos
       ‚Üì
A√ëO 2-3: Modelos mejoran, scores suben
       ‚Üì
A√ëO 4: "Saturaci√≥n" - todos scorean cerca del m√°ximo
       ‚Üì
Se necesitan nuevos benchmarks m√°s dif√≠ciles
       ‚Üì
El ciclo se repite
```

### Implicaci√≥n:

```
Modelos de HOY son significativamente mejores
que modelos de hace 2 a√±os.

El modelo que elijas HOY probablemente necesitar√°
ser reemplazado cuando salgan modelos mejores.

‚Üí Dise√±√° tu sistema para poder cambiar de modelo.
```

---

## Resumen: C√≥mo Elegir

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                                                         ‚îÇ
‚îÇ   PASO 1: Definir constraints                          ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ Presupuesto m√°ximo                               ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ Latencia requerida                               ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ Context window necesario                         ‚îÇ
‚îÇ                                                         ‚îÇ
‚îÇ   PASO 2: Filtrar por m√©tricas cuantificables          ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ Costo por token                                  ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ Velocidad                                        ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ Context window                                   ‚îÇ
‚îÇ                                                         ‚îÇ
‚îÇ   PASO 3: Comparar calidad con benchmarks relevantes   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ Elegir benchmarks que apliquen a tu use case    ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ Verificar alignment con realidad                 ‚îÇ
‚îÇ                                                         ‚îÇ
‚îÇ   PASO 4: Testear con tu data real                     ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ Ning√∫n benchmark reemplaza pruebas reales       ‚îÇ
‚îÇ                                                         ‚îÇ
‚îÇ   PASO 5: Planear para cambio                          ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ El modelo que elijas ser√° reemplazado           ‚îÇ
‚îÇ                                                         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## Aplicaci√≥n para DONA üéØ

### Consideraciones para DONA:

```
PRESUPUESTO:
‚îú‚îÄ‚îÄ ¬øCu√°ntas queries por d√≠a?
‚îú‚îÄ‚îÄ ¬øCu√°nto pod√©s gastar en LLM?
‚îî‚îÄ‚îÄ Input tokens (contexto) vs output tokens (respuesta)

LATENCIA:
‚îú‚îÄ‚îÄ Clientes esperan respuesta r√°pida
‚îú‚îÄ‚îÄ Real-time chat ‚Üí latencia importa
‚îî‚îÄ‚îÄ Time to first token cr√≠tico

CONTEXT WINDOW:
‚îú‚îÄ‚îÄ ¬øCu√°ntos productos en contexto a la vez?
‚îú‚îÄ‚îÄ ¬øManuales t√©cnicos largos?
‚îî‚îÄ‚îÄ T√≠picamente 8-16K es suficiente para DONA

CALIDAD:
‚îú‚îÄ‚îÄ Necesita entender espa√±ol
‚îú‚îÄ‚îÄ Necesita seguir instrucciones (grounding)
‚îî‚îÄ‚îÄ NO necesita c√≥digo ni matem√°tica avanzada
```

### Opciones para DONA:

```
OPCI√ìN 1: Modelo grande (GPT-4, Claude)
‚îú‚îÄ‚îÄ Mejor calidad
‚îú‚îÄ‚îÄ Mejor seguimiento de instrucciones
‚îú‚îÄ‚îÄ M√°s caro
‚îî‚îÄ‚îÄ Para: Producci√≥n con presupuesto

OPCI√ìN 2: Modelo mediano (GPT-3.5, Claude Haiku)
‚îú‚îÄ‚îÄ Buena calidad para tareas simples
‚îú‚îÄ‚îÄ Mucho m√°s barato
‚îú‚îÄ‚îÄ M√°s r√°pido
‚îî‚îÄ‚îÄ Para: Alto volumen, MVP

OPCI√ìN 3: Modelo open-source (Llama, Mistral)
‚îú‚îÄ‚îÄ Sin costo de API
‚îú‚îÄ‚îÄ Control total
‚îú‚îÄ‚îÄ Necesita infraestructura propia
‚îî‚îÄ‚îÄ Para: Control total, privacidad de datos
```

### Recomendaci√≥n para DONA:

```
FASE 1 (MVP):
‚îî‚îÄ‚îÄ GPT-3.5 o Claude Haiku (barato, r√°pido, suficiente)

FASE 2 (Producci√≥n):
‚îî‚îÄ‚îÄ Evaluar si calidad es suficiente
    Si no ‚Üí upgrade a GPT-4 / Claude Sonnet

FASE 3 (Escala):
‚îî‚îÄ‚îÄ Considerar open-source para reducir costos
```

---

## Resumen del Cap√≠tulo 30

| Factor | Qu√© considerar |
|--------|----------------|
| **Size** | M√°s grande = m√°s capaz pero m√°s caro |
| **Cost** | $ por mill√≥n de tokens |
| **Context** | M√°ximo tokens (prompt + completion) |
| **Speed** | Latencia y tokens/segundo |
| **Cutoff** | Fecha hasta donde tiene conocimiento |
| **Quality** | Benchmarks relevantes a tu use case |

---

## Key Takeaway:

> "Elegir el LLM correcto es una decisi√≥n importante pero TEMPORAL. Gracias a la velocidad con que mejoran los modelos, deber√≠as planear eventualmente cambiar a modelos nuevos que se ajusten mejor a tu sistema RAG."

---

## Pr√≥ximo: Prompting para RAG

C√≥mo estructurar prompts para que el LLM use el contexto recuperado.

---
