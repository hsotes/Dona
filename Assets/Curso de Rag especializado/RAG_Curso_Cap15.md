# Cap√≠tulo 15: Evaluaci√≥n del Retriever

---

## ¬øQu√© Importa Medir?

> "Pod√©s evaluar latencia, throughput, uso de recursos... pero lo que realmente importa es la **calidad de b√∫squeda**. ¬øEst√° encontrando documentos relevantes?"

---

## Los Ingredientes para Evaluar

```
Para evaluar un retriever necesit√°s:

1. PROMPT: La consulta del usuario
2. RETRIEVED DOCS: Lista rankeada que el retriever retorn√≥
3. GROUND TRUTH: Documentos que DEBER√çAN haberse retornado
                 (los ten√©s que marcar manualmente)
```

> "Si quer√©s calificar tu retriever, necesit√°s saber las respuestas correctas."

---

## Las Dos M√©tricas Fundamentales

### Precision (Precisi√≥n)

```
           Documentos relevantes recuperados
Precision = ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
             Total de documentos recuperados

"¬øQu√© % de lo que trajo es relevante?"
```

### Recall (Exhaustividad)

```
         Documentos relevantes recuperados
Recall = ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
         Total de documentos relevantes en la KB

"¬øQu√© % de los relevantes encontr√≥?"
```

---

## Ejemplo Pr√°ctico

### Situaci√≥n:

```
Knowledge Base tiene 10 documentos relevantes para el prompt.
(Los marcaste manualmente como ground truth)
```

### Run 1:

```
Retriever retorna: 12 documentos
Relevantes encontrados: 8

Precision = 8/12 = 66%  (8 de 12 son relevantes)
Recall = 8/10 = 80%     (encontr√≥ 8 de los 10)
```

### Run 2 (despu√©s de ajustar settings):

```
Retriever retorna: 15 documentos
Relevantes encontrados: 9

Precision = 9/15 = 60%  (baj√≥ - m√°s basura)
Recall = 9/10 = 90%     (subi√≥ - encontr√≥ m√°s)
```

> "Cambiaste un poco de precision por m√°s recall."

---

## Precision vs Recall: El Trade-off

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                                                         ‚îÇ
‚îÇ   PRECISION:                                           ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ Penaliza por retornar documentos IRRELEVANTES   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ Mide qu√© tan CONFIABLES son los resultados      ‚îÇ
‚îÇ                                                         ‚îÇ
‚îÇ   RECALL:                                              ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ Penaliza por OMITIR documentos relevantes       ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ Mide qu√© tan COMPLETO es el retriever           ‚îÇ
‚îÇ                                                         ‚îÇ
‚îÇ   PERFECTO = Rankear los relevantes arriba            ‚îÇ
‚îÇ              y SOLO retornar esos                      ‚îÇ
‚îÇ                                                         ‚îÇ
‚îÇ   EN LA PR√ÅCTICA: Hay trade-off entre ambos           ‚îÇ
‚îÇ                                                         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## Precision@K y Recall@K

### ¬øPor qu√© "@K"?

```
Las m√©tricas dependen de CU√ÅNTOS documentos retorn√°s.
Para estandarizar, se miden en "top K documentos".
```

### Ejemplo:

```
Ranking del retriever:
Rank 1: ‚úÖ Relevante
Rank 2: ‚ùå Irrelevante
Rank 3: ‚ùå Irrelevante
Rank 4: ‚úÖ Relevante
Rank 5: ‚úÖ Relevante
Rank 6: ‚ùå Irrelevante
Rank 7: ‚úÖ Relevante
Rank 8: ‚ùå Irrelevante
Rank 9: ‚úÖ Relevante
Rank 10: ‚úÖ Relevante

Precision@5 = 3/5 = 60%   (3 relevantes en top 5)
Precision@10 = 6/10 = 60% (6 relevantes en top 10)

Si hay 8 relevantes en total en la KB:
Recall@10 = 6/8 = 75%     (encontr√≥ 6 de 8)
```

### Valores t√≠picos de K:

```
Estricto: @1, @2, @5
Generoso: @5 a @15 (m√°s com√∫n)
```

---

## MAP@K (Mean Average Precision)

### ¬øQu√© mide?

```
Eval√∫a el PROMEDIO de precision para documentos relevantes.
Premia rankear los relevantes ARRIBA.
```

### C√°lculo paso a paso:

```
Top 6 documentos del retriever:

Rank | Relevante? | Precision@K
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
  1  |     ‚úÖ     | 1/1 = 1.0   ‚Üê sumar
  2  |     ‚ùå     | 1/2 = 0.5
  3  |     ‚ùå     | 1/3 = 0.33
  4  |     ‚úÖ     | 2/4 = 0.5   ‚Üê sumar
  5  |     ‚úÖ     | 3/5 = 0.6   ‚Üê sumar
  6  |     ‚ùå     | 3/6 = 0.5

Solo sumamos las filas con documentos relevantes:
1.0 + 0.5 + 0.6 = 2.1

Dividimos por cantidad de relevantes encontrados (3):
AP@6 = 2.1 / 3 = 0.7

MAP = Promedio de AP sobre m√∫ltiples prompts
```

### Por qu√© MAP es √∫til:

```
Si un documento irrelevante se cuela arriba en el ranking,
baja la precision de TODOS los relevantes debajo de √©l.

Alto MAP = relevantes est√°n bien rankeados arriba
```

---

## MRR (Mean Reciprocal Rank)

### ¬øQu√© mide?

```
La posici√≥n del PRIMER documento relevante.

"¬øQu√© tan r√°pido encontr√°s algo √∫til?"
```

### F√≥rmula:

```
Reciprocal Rank = 1 / (posici√≥n del primer relevante)

Si el primer relevante est√° en:
‚îú‚îÄ‚îÄ Rank 1 ‚Üí RR = 1/1 = 1.0
‚îú‚îÄ‚îÄ Rank 2 ‚Üí RR = 1/2 = 0.5
‚îú‚îÄ‚îÄ Rank 4 ‚Üí RR = 1/4 = 0.25
‚îî‚îÄ‚îÄ Rank 6 ‚Üí RR = 1/6 = 0.17
```

### MRR sobre m√∫ltiples prompts:

```
4 b√∫squedas, primer relevante en posiciones: 1, 3, 6, 2

RR de cada una: 1, 1/3, 1/6, 1/2

MRR = (1 + 0.33 + 0.17 + 0.5) / 4 = 0.5
```

### Cu√°ndo usar MRR:

```
Cuando importa tener AL MENOS UN relevante lo m√°s arriba posible.
```

---

## Resumen de M√©tricas

| M√©trica | Qu√© mide | Cu√°ndo usar |
|---------|----------|-------------|
| **Recall@K** | % de relevantes encontrados | Siempre (la m√°s fundamental) |
| **Precision@K** | % de resultados que son relevantes | Cuando importa no traer basura |
| **MAP@K** | Promedio de precision en relevantes | Cuando importa el ranking |
| **MRR** | Posici√≥n del primer relevante | Cuando un buen resultado basta |

---

## C√≥mo Usar las M√©tricas

### Para evaluar y mejorar:

```python
# Ejemplo: Comparar configuraciones de hybrid search

config_a = {"beta": 0.7}  # 70% semantic
config_b = {"beta": 0.5}  # 50% semantic
config_c = {"beta": 0.3}  # 30% semantic

# Correr sobre conjunto de prompts de prueba con ground truth

results_a = evaluate(config_a, test_prompts)
results_b = evaluate(config_b, test_prompts)
results_c = evaluate(config_c, test_prompts)

# Comparar
print(f"Config A - Recall@5: {results_a.recall_5}, Precision@5: {results_a.precision_5}")
print(f"Config B - Recall@5: {results_b.recall_5}, Precision@5: {results_b.precision_5}")
print(f"Config C - Recall@5: {results_c.recall_5}, Precision@5: {results_c.precision_5}")

# Elegir la mejor configuraci√≥n
```

---

## El Desaf√≠o: Ground Truth

### El problema:

```
‚ùå Todas las m√©tricas requieren GROUND TRUTH
‚ùå Alguien tiene que marcar manualmente qu√© docs son relevantes
‚ùå Es un proceso lento y tedioso
```

### La recompensa:

```
‚úÖ Sistema que pod√©s monitorear durante desarrollo
‚úÖ Sistema que pod√©s monitorear en producci√≥n
‚úÖ Sab√©s si tus cambios mejoran o empeoran las cosas
```

---

## Aplicaci√≥n para DONA üéØ

### Crear un test set:

```
1. Seleccionar 20-50 prompts reales de usuarios
2. Para cada prompt, marcar manualmente:
   "¬øQu√© productos DEBER√çAN aparecer?"

Ejemplo:
Prompt: "hierro del 8 acindar"
Ground Truth: [prod_001, prod_002, prod_015]

Prompt: "cemento para revocar"
Ground Truth: [prod_100, prod_101, prod_105, prod_108]
```

### M√©tricas para DONA:

```
RECALL@5: ¬øEncuentra los productos correctos?
          ‚Üí Lo m√°s importante para DONA

PRECISION@5: ¬øTrae mucha basura?
             ‚Üí Importante pero secundario

MRR: ¬øEl producto correcto est√° arriba?
     ‚Üí Importante para UX
```

### Diagn√≥stico con m√©tricas:

```
Si Recall@5 es bajo:
‚îú‚îÄ‚îÄ Problema de embeddings
‚îú‚îÄ‚îÄ Problema de chunking
‚îî‚îÄ‚îÄ Documentos no est√°n en la KB

Si Precision@5 es bajo:
‚îú‚îÄ‚îÄ Trae muchos docs irrelevantes
‚îú‚îÄ‚îÄ Ajustar metadata filtering
‚îî‚îÄ‚îÄ Ajustar beta (keyword vs semantic)

Si MRR es bajo:
‚îú‚îÄ‚îÄ Los relevantes est√°n muy abajo
‚îú‚îÄ‚îÄ Ajustar RRF parameters
‚îî‚îÄ‚îÄ Revisar ranking
```

---

## Resumen del Cap√≠tulo 15

| Concepto | Explicaci√≥n |
|----------|-------------|
| **Precision** | % de resultados que son relevantes |
| **Recall** | % de relevantes que encontr√≥ |
| **@K** | M√©tricas sobre top K documentos |
| **MAP** | Promedio de precision (premia buen ranking) |
| **MRR** | Posici√≥n del primer relevante |
| **Ground Truth** | Respuestas correctas marcadas manualmente |

---

## Key Takeaway:

> "Recall es la m√©trica m√°s fundamental porque captura el objetivo m√°s b√°sico del retriever: encontrar documentos relevantes."

---

## Pr√≥ximo: Wrap-up M√≥dulo 2

Resumen de todas las t√©cnicas de Information Retrieval.

---
