# CapÃ­tulo 34: EvaluaciÃ³n de LLMs en RAG

---

## La Necesidad de MÃ©tricas

> "Ya sea que acabas de construir tu primer proof-of-concept RAG o estÃ¡s iterando en un sistema en producciÃ³n, vas a querer saber quÃ© tan bien estÃ¡ funcionando tu LLM."

### Decisiones que requieren mÃ©tricas:

```
â”œâ”€â”€ Â¿Ajustar el temperature del modelo?
â”œâ”€â”€ Â¿Actualizar el system prompt?
â”œâ”€â”€ Â¿Cambiar a un modelo completamente nuevo?

Para tomar decisiones informadas,
necesitÃ¡s mÃ©tricas para medir el impacto de cada cambio.
```

---

## Separar Responsabilidades

### Clarificar quÃ© evaluar:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                         â”‚
â”‚   RETRIEVER:                                           â”‚
â”‚   Encontrar informaciÃ³n relevante de la knowledge base â”‚
â”‚                                                         â”‚
â”‚   LLM:                                                 â”‚
â”‚   Usar esa informaciÃ³n para construir una respuesta    â”‚
â”‚   de alta calidad                                      â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Por quÃ© importa:

```
Si el problema estÃ¡ en el RETRIEVER:
â†’ No querÃ©s perder tiempo reescribiendo el system prompt

Si el problema estÃ¡ en el LLM:
â†’ No querÃ©s perder tiempo ajustando embeddings

Las mÃ©tricas deben enfocarse en el ROL ESPECÃFICO
de cada componente en el pipeline.
```

---

## El Trabajo EspecÃ­fico del LLM

### Asumiendo que el retriever funciona bien:

```
El retriever trae:
â”œâ”€â”€ InformaciÃ³n mayormente relevante
â””â”€â”€ QuizÃ¡s algunos docs irrelevantes

El LLM debe:
â”œâ”€â”€ Responder al prompt del usuario
â”œâ”€â”€ Incorporar la informaciÃ³n relevante
â”œâ”€â”€ Citar apropiadamente
â””â”€â”€ RESISTIR distraerse con info irrelevante
```

---

## El DesafÃ­o de Evaluar LLMs

### La naturaleza subjetiva:

```
Â¿CÃ³mo decÃ­s CUANTITATIVAMENTE que una respuesta...

â”œâ”€â”€ "Hace un buen trabajo respondiendo la pregunta"?
â”œâ”€â”€ "Ignora correctamente la info irrelevante"?
â”œâ”€â”€ "Incorpora bien la informaciÃ³n recuperada"?

Estas son evaluaciones SUBJETIVAS.
```

### La soluciÃ³n:

```
La mayorÃ­a de mÃ©tricas especÃ­ficas de LLM
usan OTROS LLMs para evaluar la calidad.

LLM-as-a-Judge permite:
â”œâ”€â”€ Cierto grado de flexibilidad/subjetividad
â””â”€â”€ De manera ESCALABLE
```

---

## RAGAS Library

### LibrerÃ­a open-source para mÃ©tricas RAG:

```
RAGAS = RAG Assessment
Provee mÃ©tricas especÃ­ficas para evaluar sistemas RAG.
```

---

## MÃ©trica 1: Response Relevancy

### QuÃ© mide:

```
Â¿La respuesta es RELEVANTE al prompt del usuario?

(Independiente de si es factualmente precisa)
```

### CÃ³mo funciona:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                         â”‚
â”‚   PASO 1: Tomar la respuesta de tu RAG system          â”‚
â”‚                                                         â”‚
â”‚   PASO 2: Otro LLM genera varios "sample prompts"      â”‚
â”‚           que PODRÃAN haber generado esa respuesta     â”‚
â”‚                                                         â”‚
â”‚   PASO 3: Embeber el prompt REAL y los sample prompts  â”‚
â”‚                                                         â”‚
â”‚   PASO 4: Calcular cosine similarity entre             â”‚
â”‚           el prompt real y cada sample prompt          â”‚
â”‚                                                         â”‚
â”‚   PASO 5: Promediar los scores de similaridad          â”‚
â”‚           = Response Relevancy Score                   â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### VisualizaciÃ³n:

```
Usuario pregunta: "Â¿CuÃ¡nto cuesta el cemento?"

Tu RAG responde: "El cemento Portland estÃ¡ a $8,500 
                 la bolsa de 50kg."

LLM genera sample prompts:
â”œâ”€â”€ "Â¿CuÃ¡l es el precio del cemento Portland?"
â”œâ”€â”€ "Â¿CuÃ¡nto sale una bolsa de cemento?"
â””â”€â”€ "Â¿El cemento estÃ¡ caro?"

Similaridad con prompt real:
â”œâ”€â”€ Sample 1 â†” Real: 0.92
â”œâ”€â”€ Sample 2 â†” Real: 0.85
â””â”€â”€ Sample 3 â†” Real: 0.70

Response Relevancy = promedio = 0.82
```

### Lo que NO mide:

```
âš ï¸ NO garantiza que la respuesta sea factualmente correcta
âš ï¸ Solo verifica que podÃ©s "trabajar hacia atrÃ¡s" 
   desde la respuesta al prompt original
```

---

## MÃ©trica 2: Faithfulness

### QuÃ© mide:

```
Â¿El LLM estÃ¡ realmente USANDO la informaciÃ³n recuperada?
```

### CÃ³mo funciona:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                         â”‚
â”‚   PASO 1: LLM identifica todos los CLAIMS factuales    â”‚
â”‚           en la respuesta                              â”‚
â”‚                                                         â”‚
â”‚   PASO 2: Para cada claim, LLM determina si estÃ¡       â”‚
â”‚           SOPORTADO por algÃºn documento recuperado     â”‚
â”‚                                                         â”‚
â”‚   PASO 3: Calcular porcentaje de claims soportados     â”‚
â”‚           = Faithfulness Score                         â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Ejemplo:

```
Respuesta del LLM:
"El cemento Portland cuesta $8,500 [claim 1].
 Es ideal para construcciÃ³n general [claim 2].
 Tenemos stock disponible [claim 3]."

VerificaciÃ³n contra documentos:
â”œâ”€â”€ Claim 1: "$8,500" â†’ âœ… Soportado por Doc 1
â”œâ”€â”€ Claim 2: "construcciÃ³n general" â†’ âœ… Soportado por Doc 1
â”œâ”€â”€ Claim 3: "stock disponible" â†’ âŒ No mencionado en docs

Faithfulness = 2/3 = 0.67
```

---

## Otras MÃ©tricas en RAGAS

### MÃ©tricas adicionales:

```
â”œâ”€â”€ NOISE SENSITIVITY
â”‚   Â¿QuÃ© tan bien resiste info irrelevante?
â”‚
â”œâ”€â”€ CITATION ACCURACY
â”‚   Â¿Las citas son precisas y correctas?
â”‚
â”œâ”€â”€ CONTEXT UTILIZATION
â”‚   Â¿QuÃ© proporciÃ³n del contexto relevante usa?
â”‚
â””â”€â”€ ANSWER CORRECTNESS
    Â¿La respuesta es factualmente correcta?
    (Requiere ground truth)
```

---

## El PatrÃ³n ComÃºn

### Todas las mÃ©tricas de LLM:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                         â”‚
â”‚   Dependencia de:                                      â”‚
â”‚   â”œâ”€â”€ Llamadas a LLM en algÃºn punto del eval          â”‚
â”‚   â””â”€â”€ Posiblemente ejemplos de ground truth           â”‚
â”‚                                                         â”‚
â”‚   Esto refleja que el rol del LLM es:                  â”‚
â”‚   â”œâ”€â”€ Complejo                                         â”‚
â”‚   â””â”€â”€ DifÃ­cil de evaluar con mÃ©tricas automatizadas   â”‚
â”‚       simples                                          â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## MÃ©tricas a Nivel de Sistema

### A/B Testing con feedback de usuarios:

```
Si tus usuarios pueden dar thumbs up/down:

PASO 1: Medir baseline (ej: 75% thumbs up)

PASO 2: Cambiar SOLO algo del LLM 
        (ej: nuevo system prompt)

PASO 3: Medir nuevo performance (ej: 82% thumbs up)

PASO 4: Atribuir el cambio al LLM
        (porque fue lo Ãºnico que cambiÃ³)
```

### La idea:

```
MedÃ­s performance de TODO el sistema
PERO aislÃ¡s cambios a settings del LLM

â†’ PodÃ©s atribuir cambios en performance
  a cambios en tu LLM
```

---

## Resumen de MÃ©tricas

| MÃ©trica | QuÃ© evalÃºa | CÃ³mo funciona |
|---------|------------|---------------|
| **Response Relevancy** | Â¿Respuesta relevante al prompt? | Reverse-engineer prompts, medir similaridad |
| **Faithfulness** | Â¿Usa info recuperada? | Verificar claims contra docs |
| **Noise Sensitivity** | Â¿Resiste info irrelevante? | Test con ruido agregado |
| **Citation Accuracy** | Â¿Citas correctas? | Verificar cada cita |
| **User Feedback** | Â¿Usuarios satisfechos? | Thumbs up/down, A/B tests |

---

## AplicaciÃ³n para DONA ğŸ¯

### Setup de evaluaciÃ³n para DONA:

```python
from ragas import evaluate
from ragas.metrics import (
    response_relevancy,
    faithfulness,
    answer_relevancy
)

def evaluate_dona_response(query, response, retrieved_docs):
    """Evaluar una respuesta de DONA"""
    
    # Preparar datos para RAGAS
    eval_data = {
        "question": query,
        "answer": response,
        "contexts": [doc['contenido'] for doc in retrieved_docs]
    }
    
    # Evaluar
    result = evaluate(
        dataset=eval_data,
        metrics=[
            response_relevancy,
            faithfulness
        ]
    )
    
    return result
```

### MÃ©tricas especÃ­ficas para DONA:

```python
def dona_specific_metrics(query, response, retrieved_docs):
    """MÃ©tricas especÃ­ficas para el dominio de DONA"""
    
    metrics = {}
    
    # 1. Â¿MencionÃ³ precio si estaba disponible?
    precios_en_docs = any('precio' in doc['contenido'].lower() 
                         for doc in retrieved_docs)
    precio_en_respuesta = '$' in response
    
    if precios_en_docs:
        metrics['price_inclusion'] = 1.0 if precio_en_respuesta else 0.0
    
    # 2. Â¿MencionÃ³ disponibilidad?
    stock_en_docs = any('stock' in doc['contenido'].lower() 
                       for doc in retrieved_docs)
    stock_en_respuesta = 'disponib' in response.lower() or 'stock' in response.lower()
    
    if stock_en_docs:
        metrics['availability_mention'] = 1.0 if stock_en_respuesta else 0.0
    
    # 3. Â¿PreguntÃ³ cantidad (buena prÃ¡ctica de ventas)?
    metrics['asks_quantity'] = 1.0 if 'Â¿cuÃ¡nt' in response.lower() else 0.0
    
    return metrics
```

### A/B Testing para DONA:

```python
def ab_test_system_prompt(prompt_a, prompt_b, test_queries):
    """A/B test de dos system prompts"""
    
    results = {"prompt_a": [], "prompt_b": []}
    
    for query in test_queries:
        # Generar respuestas con ambos prompts
        response_a = generate_with_prompt(query, prompt_a)
        response_b = generate_with_prompt(query, prompt_b)
        
        # Evaluar con RAGAS
        score_a = evaluate_response(query, response_a)
        score_b = evaluate_response(query, response_b)
        
        results["prompt_a"].append(score_a)
        results["prompt_b"].append(score_b)
    
    # Comparar promedios
    avg_a = sum(results["prompt_a"]) / len(results["prompt_a"])
    avg_b = sum(results["prompt_b"]) / len(results["prompt_b"])
    
    print(f"Prompt A: {avg_a:.2f}")
    print(f"Prompt B: {avg_b:.2f}")
    
    return results
```

### Dashboard de mÃ©tricas para DONA:

```python
DONA_METRICS_DASHBOARD = {
    # MÃ©tricas RAG estÃ¡ndar
    "response_relevancy": {
        "target": 0.85,
        "alert_below": 0.70
    },
    "faithfulness": {
        "target": 0.90,
        "alert_below": 0.75
    },
    
    # MÃ©tricas especÃ­ficas de DONA
    "price_inclusion": {
        "target": 0.95,
        "alert_below": 0.80
    },
    "availability_mention": {
        "target": 0.90,
        "alert_below": 0.75
    },
    
    # Feedback de usuarios
    "user_satisfaction": {
        "target": 0.85,
        "alert_below": 0.70
    }
}
```

---

## Resumen del CapÃ­tulo 34

| Concepto | ExplicaciÃ³n |
|----------|-------------|
| **Separar responsabilidades** | Retriever vs LLM tienen mÃ©tricas diferentes |
| **Response Relevancy** | Â¿Respuesta es relevante al prompt? |
| **Faithfulness** | Â¿LLM usa la info recuperada? |
| **LLM-as-Judge** | Usar LLMs para evaluar (subjetivo pero escalable) |
| **A/B Testing** | Aislar cambios de LLM, medir impacto |

---

## Key Takeaways:

```
1. El LLM tiene responsabilidades ESPECÃFICAS distintas al retriever

2. MÃ©tricas de LLM son inherentemente SUBJETIVAS
   â†’ Requieren LLM-as-judge o feedback humano

3. RAGAS provee mÃ©tricas Ãºtiles:
   â”œâ”€â”€ Response Relevancy
   â””â”€â”€ Faithfulness

4. A/B testing permite evaluar cambios especÃ­ficos

5. CombinÃ¡ LLM-as-judge + feedback humano
   para evaluaciÃ³n confiable
```

---

## PrÃ³ximo: Wrap-up MÃ³dulo 4

Resumen de todo lo aprendido sobre LLMs en RAG.

---
