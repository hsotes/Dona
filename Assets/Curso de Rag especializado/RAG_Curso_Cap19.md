# CapÃ­tulo 19: Algoritmos de Vector Search

---

## El Problema de Escala

> "Si se implementa ingenuamente, vector search escala muy mal, requiriendo recursos computacionales significativos y agregando latencia al sistema."

---

## K-Nearest Neighbors (KNN)

### El algoritmo mÃ¡s simple:

```
1. Crear embedding para cada documento
2. Crear embedding para el prompt
3. Calcular distancia entre prompt y CADA documento
4. Ordenar por distancia
5. Retornar los K mÃ¡s cercanos
```

### El problema:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                         â”‚
â”‚   KNN ESCALA TERRIBLEMENTE                             â”‚
â”‚                                                         â”‚
â”‚   El nÃºmero de cÃ¡lculos crece LINEALMENTE              â”‚
â”‚   con el nÃºmero de documentos.                         â”‚
â”‚                                                         â”‚
â”‚   1,000 documentos â†’ 1,000 cÃ¡lculos de distancia       â”‚
â”‚   1,000,000 documentos â†’ 1,000,000 cÃ¡lculos            â”‚
â”‚   1,000,000,000 documentos â†’ 1,000,000,000 cÃ¡lculos    â”‚
â”‚                                                         â”‚
â”‚   La segunda bÃºsqueda serÃ¡ UN MILLÃ“N de veces          â”‚
â”‚   mÃ¡s lenta que la primera.                            â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Approximate Nearest Neighbors (ANN)

### La soluciÃ³n:

> "Familia de algoritmos que usan estructuras de datos inteligentes para bÃºsquedas significativamente mÃ¡s rÃ¡pidas."

### El trade-off:

```
KNN: Garantiza encontrar los MÃS cercanos
     Pero muy lento

ANN: No garantiza los ABSOLUTOS mÃ¡s cercanos
     Pero encuentra vectores MUY cercanos
     Y es MUCHO mÃ¡s rÃ¡pido
```

---

## Navigable Small World (NSW)

### Paso 1: Construir el Proximity Graph

```
ANTES de cualquier bÃºsqueda:

1. Calcular distancia entre CADA par de vectores
2. Crear un NODO por cada documento
3. Crear EDGES entre cada documento y sus vecinos mÃ¡s cercanos

Resultado: Estructura tipo red/web
```

### VisualizaciÃ³n del Proximity Graph:

```
        â€¢ doc1 â”€â”€â”€â”€â”€â”€â”€ â€¢ doc2
       / \              |
      /   \             |
     /     \            |
    â€¢ doc3   â€¢ doc4 â”€â”€â”€â€¢ doc5
     \      /     \    /
      \    /       \  /
       \  /         \/
        â€¢ doc6 â”€â”€â”€â”€â”€ â€¢ doc7
        
Cada documento conectado a sus vecinos mÃ¡s cercanos
```

---

### Paso 2: BÃºsqueda en el Graph

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                         â”‚
â”‚   1. Recibir prompt â†’ crear query vector               â”‚
â”‚                                                         â”‚
â”‚   2. Elegir punto de entrada ALEATORIO                 â”‚
â”‚      (no asume que estÃ¡ cerca del prompt)              â”‚
â”‚                                                         â”‚
â”‚   3. Mirar los VECINOS del nodo actual                 â”‚
â”‚      (pocos vecinos = muy rÃ¡pido)                      â”‚
â”‚                                                         â”‚
â”‚   4. Â¿CuÃ¡l vecino estÃ¡ MÃS CERCA del prompt?          â”‚
â”‚      â†’ Ese se vuelve el nuevo candidato                â”‚
â”‚                                                         â”‚
â”‚   5. REPETIR paso 3-4                                  â”‚
â”‚                                                         â”‚
â”‚   6. PARAR cuando ningÃºn vecino estÃ¡ mÃ¡s cerca         â”‚
â”‚      que el candidato actual                           â”‚
â”‚                                                         â”‚
â”‚   7. Retornar el candidato final                       â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Ejemplo visual:

```
Query vector: â˜…

Paso 1: Entrada aleatoria en doc1
        doc1 â€¢ â”€â”€â”€â”€â”€â”€â”€ â€¢ doc2
             â”‚
             â”‚  â˜… (query estÃ¡ por acÃ¡)
             â”‚
        doc3 â€¢ â”€â”€â”€â”€â”€â”€â”€ â€¢ doc4

Paso 2: Vecinos de doc1: doc2, doc3
        doc3 estÃ¡ mÃ¡s cerca de â˜… â†’ moverse a doc3

Paso 3: Vecinos de doc3: doc1, doc4
        doc4 estÃ¡ mÃ¡s cerca de â˜… â†’ moverse a doc4

Paso 4: Vecinos de doc4: doc3, doc5
        Ninguno mÃ¡s cerca â†’ RETORNAR doc4
```

---

### Por quÃ© no es perfecto pero funciona:

```
âŒ Puede que exista un documento MÃS cercano
   que el algoritmo nunca alcanza
   (porque no puede elegir el path Ã³ptimo global,
    solo el mejor en cada momento)

âœ… En la prÃ¡ctica encuentra vectores MUY cercanos
âœ… MUCHO mÃ¡s rÃ¡pido que KNN
```

---

## HNSW: Hierarchical Navigable Small World

### La mejora sobre NSW:

> "Agrega capas jerÃ¡rquicas para acelerar significativamente las primeras partes de la bÃºsqueda."

### Estructura jerÃ¡rquica:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                         â”‚
â”‚   LAYER 3 (top): Solo 10 vectores                      â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚   â”‚    â€¢       â€¢       â€¢                â”‚              â”‚
â”‚   â”‚      \   /   \   /                  â”‚              â”‚
â”‚   â”‚       â€¢ â”€â”€â”€â”€â”€ â€¢                     â”‚              â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚                    â†“                                    â”‚
â”‚   LAYER 2: 100 vectores                                â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚   â”‚  â€¢ â€¢ â€¢   â€¢ â€¢ â€¢   â€¢ â€¢ â€¢   â€¢ â€¢       â”‚              â”‚
â”‚   â”‚   \|/     \|/     \|/     \/       â”‚              â”‚
â”‚   â”‚    â€¢       â€¢       â€¢      â€¢        â”‚              â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚                    â†“                                    â”‚
â”‚   LAYER 1 (bottom): TODOS los 1000 vectores           â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚   â”‚  â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢  â”‚              â”‚
â”‚   â”‚  â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢  â”‚              â”‚
â”‚   â”‚  â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢  â”‚              â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### CÃ³mo funciona la bÃºsqueda HNSW:

```
1. EMPEZAR en Layer 3 (solo 10 vectores)
   â”œâ”€â”€ Elegir entrada aleatoria
   â”œâ”€â”€ Navegar hasta encontrar mejor candidato
   â””â”€â”€ SALTOS GRANDES hacia el vecindario correcto

2. BAJAR a Layer 2 (100 vectores)
   â”œâ”€â”€ Empezar desde el mejor de Layer 3
   â”œâ”€â”€ Navegar para encontrar mejor candidato
   â””â”€â”€ Refinando la ubicaciÃ³n

3. BAJAR a Layer 1 (TODOS los vectores)
   â”œâ”€â”€ Empezar desde el mejor de Layer 2
   â”œâ”€â”€ Ya estamos MUY CERCA del prompt
   â””â”€â”€ Navegar para encontrar el mejor final

4. RETORNAR el candidato final
```

---

### Por quÃ© HNSW es tan eficiente:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                         â”‚
â”‚   CAPAS SUPERIORES:                                    â”‚
â”‚   â”œâ”€â”€ Pocos vectores                                   â”‚
â”‚   â”œâ”€â”€ Saltos GRANDES                                   â”‚
â”‚   â””â”€â”€ Llegan rÃ¡pido al "vecindario" correcto          â”‚
â”‚                                                         â”‚
â”‚   CAPA INFERIOR:                                       â”‚
â”‚   â”œâ”€â”€ Todos los vectores                               â”‚
â”‚   â”œâ”€â”€ PERO ya empezÃ¡s muy cerca del prompt            â”‚
â”‚   â””â”€â”€ Solo refinamiento final                          â”‚
â”‚                                                         â”‚
â”‚   COMPLEJIDAD:                                         â”‚
â”‚   KNN:  O(n) - lineal                                  â”‚
â”‚   HNSW: O(log n) - logarÃ­tmica                        â”‚
â”‚                                                         â”‚
â”‚   1 billÃ³n de vectores:                                â”‚
â”‚   KNN:  1,000,000,000 comparaciones                   â”‚
â”‚   HNSW: ~30 comparaciones                              â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Resumen de Algoritmos

| Algoritmo | Velocidad | PrecisiÃ³n | Uso |
|-----------|-----------|-----------|-----|
| **KNN** | Muy lento (O(n)) | Perfecta | Solo para datasets pequeÃ±os |
| **NSW** | RÃ¡pido | Aproximada | Datasets medianos |
| **HNSW** | Muy rÃ¡pido (O(log n)) | Aproximada | ProducciÃ³n a escala |

---

## Key Takeaways

### 1. ANN es significativamente mÃ¡s rÃ¡pido que KNN

```
Permite vector search a escala de BILLONES de vectores
con solo unos cientos de milisegundos de latencia.
```

### 2. No garantiza los matches absolutos mejores

```
Pero encuentra vectores MUY cercanos.
El trade-off vale la pena para la velocidad.
```

### 3. Depende de construir un buen Proximity Graph

```
Proceso computacionalmente intensivo
PERO se puede pre-computar antes de recibir queries.
```

---

## AplicaciÃ³n para DONA ğŸ¯

### No necesitÃ¡s implementar esto:

```
Las Vector Databases (Weaviate, Pinecone, etc.)
implementan HNSW internamente.

Solo necesitÃ¡s:
â”œâ”€â”€ Elegir una Vector DB
â”œâ”€â”€ Cargar tus documentos
â””â”€â”€ Ejecutar queries
```

### Lo que SÃ necesitÃ¡s entender:

```
1. El retrieval es APROXIMADO (no perfecto)
   â†’ Por eso usamos hybrid search

2. El Ã­ndice se construye UNA VEZ
   â†’ Agregar documentos puede requerir re-indexar

3. MÃ¡s capas = mÃ¡s rÃ¡pido pero mÃ¡s memoria
   â†’ Trade-off a configurar
```

---

## PrÃ³ximo: Herramientas para Implementar Vector Search

Vector Databases en la prÃ¡ctica.

---
