# CapÃ­tulo 45: Optimizando Latencia

---

## El Balance Latencia vs Calidad

> "Otro acto de balance importante que tenÃ©s que considerar para una aplicaciÃ³n RAG en producciÃ³n es el tiempo que toma cada query (latencia) versus la calidad de respuesta."

---

## Por QuÃ© Importa el Contexto

### Cada aplicaciÃ³n tiene diferentes tolerancias:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                         â”‚
â”‚   E-COMMERCE (recomendaciones):                        â”‚
â”‚   â”œâ”€â”€ Clientes tienen POCA PACIENCIA                   â”‚
â”‚   â”œâ”€â”€ Optimizar para BAJA LATENCIA                     â”‚
â”‚   â””â”€â”€ OK si no es la recomendaciÃ³n "perfecta"          â”‚
â”‚                                                         â”‚
â”‚   DIAGNÃ“STICO MÃ‰DICO (enfermedades raras):             â”‚
â”‚   â”œâ”€â”€ Calidad es CRÃTICA                               â”‚
â”‚   â”œâ”€â”€ Optimizar para CALIDAD                           â”‚
â”‚   â””â”€â”€ OK si tarda mÃ¡s en responder                     â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## El Culpable Principal: Transformers

### Regla fÃ¡cil de recordar:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                         â”‚
â”‚   CASI TODA la latencia viene de correr TRANSFORMERS   â”‚
â”‚                                                         â”‚
â”‚   El MAYOR CULPABLE: LLM calls                        â”‚
â”‚                                                         â”‚
â”‚   Retrieval agrega un poco de latencia, pero:          â”‚
â”‚   â”œâ”€â”€ Databases modernas son MUY RÃPIDAS              â”‚
â”‚   â””â”€â”€ Vector databases escalan bien                    â”‚
â”‚                                                         â”‚
â”‚   Si querÃ©s reducir latencia:                          â”‚
â”‚   â†’ EmpezÃ¡ por tu CORE LANGUAGE MODEL                 â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Estrategia 1: Modelos MÃ¡s PequeÃ±os

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                         â”‚
â”‚   Smaller LLMs o Quantized models                      â”‚
â”‚   = SIEMPRE corren mÃ¡s rÃ¡pido en el mismo hardware     â”‚
â”‚                                                         â”‚
â”‚   Trade-off: Posible pÃ©rdida de calidad                â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Estrategia 2: Router LLM Inteligente

### La idea:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                         â”‚
â”‚   Router LLM (pequeÃ±o y rÃ¡pido):                       â”‚
â”‚   Mira el prompt y decide quÃ© modelo usar.             â”‚
â”‚                                                         â”‚
â”‚   QUERY COMPLEJA (razonamiento):                       â”‚
â”‚   â†’ Modelo grande y poderoso (mÃ¡s lento)               â”‚
â”‚                                                         â”‚
â”‚   QUERY SIMPLE:                                        â”‚
â”‚   â†’ Modelo pequeÃ±o y rÃ¡pido                            â”‚
â”‚                                                         â”‚
â”‚   RESULTADO:                                           â”‚
â”‚   â”œâ”€â”€ Latencia baja para prompts simples              â”‚
â”‚   â””â”€â”€ Latencia alta SOLO cuando se necesita           â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Estrategia 3: Caching de Respuestas

### Para sistemas con prompts similares:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                         â”‚
â”‚   MANTENER CACHE de:                                   â”‚
â”‚   â”œâ”€â”€ Prompts frecuentes                               â”‚
â”‚   â””â”€â”€ Sus respuestas                                   â”‚
â”‚                                                         â”‚
â”‚   CUANDO llega un nuevo prompt:                        â”‚
â”‚   1. Calcular similarity con prompts en cache          â”‚
â”‚   2. Si hay match cercano â†’ retornar respuesta cacheadaâ”‚
â”‚   3. SKIP el proceso de generaciÃ³n (lento)             â”‚
â”‚                                                         â”‚
â”‚   Con tuning cuidadoso: GRAN mejora de latencia        â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Variante: Cache + PersonalizaciÃ³n

```
1. Recuperar respuesta cacheada
2. Pasar cached response + user prompt a LLM pequeÃ±o
3. Hacer pequeÃ±os ajustes para personalizar

= Respuesta rÃ¡pida + algo de personalizaciÃ³n
```

---

## Estrategia 4: Evaluar Otros Componentes Transformer

### DespuÃ©s de optimizar el core LLM:

```
OTROS COMPONENTES QUE AGREGAN LATENCIA:
â”œâ”€â”€ Query rewriter
â”œâ”€â”€ Re-ranker
â”œâ”€â”€ Router LLM
â””â”€â”€ Citation generator

PARA CADA UNO, MEDIR:
â”œâ”€â”€ Latencia que agrega
â””â”€â”€ Calidad incremental que provee

SI NO DA MUCHO BENEFICIO:
â†’ Considerar REMOVER el componente
```

### Ejemplo:

```
Query rewriter agrega 200ms pero 
solo mejora relevancy un 2%...

Â¿Vale la pena? QuizÃ¡s no.
â†’ Remover para ganar 200ms
```

---

## Estrategia 5: Optimizar Retrieval

### Aunque retrieval es generalmente rÃ¡pido:

```
BINARY QUANTIZATION de embeddings:
â”œâ”€â”€ Simplifica cÃ¡lculos de distancia
â””â”€â”€ Acelera bÃºsqueda

SHARDING de databases grandes:
â”œâ”€â”€ Separar en instancias
â””â”€â”€ Reduce latencia de bÃºsqueda

La mayorÃ­a de vector DB providers
tienen tools para implementar esto.
```

---

## Orden de Prioridad para Reducir Latencia

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                         â”‚
â”‚   1. CORE LLM (mayor impacto)                          â”‚
â”‚      â”œâ”€â”€ Modelo mÃ¡s pequeÃ±o                            â”‚
â”‚      â”œâ”€â”€ Quantization                                  â”‚
â”‚      â”œâ”€â”€ Router para elegir modelo                     â”‚
â”‚      â””â”€â”€ Caching                                       â”‚
â”‚                                                         â”‚
â”‚   2. OTROS TRANSFORMERS                                â”‚
â”‚      â”œâ”€â”€ Medir latencia de cada uno                   â”‚
â”‚      â”œâ”€â”€ Medir beneficio de calidad                   â”‚
â”‚      â””â”€â”€ Remover si no vale la pena                   â”‚
â”‚                                                         â”‚
â”‚   3. RETRIEVAL (si todavÃ­a hay issues)                â”‚
â”‚      â”œâ”€â”€ Binary quantization                          â”‚
â”‚      â””â”€â”€ Sharding                                      â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## AplicaciÃ³n para DONA ðŸŽ¯

### AnÃ¡lisis de latencia para DONA:

```python
DONA_LATENCY_BREAKDOWN = {
    # TÃ­pico query flow
    "components": {
        "intent_detection": 50,    # ms
        "query_rewriting": 80,     # ms
        "vector_search": 100,      # ms
        "re_ranking": 120,         # ms
        "llm_generation": 1200,    # ms â† MAYOR CULPABLE
        "response_formatting": 30  # ms
    },
    "total_typical": 1580,  # ms
    "target": 2000,         # ms (aceptable para chat)
    "stretch_target": 1000  # ms (ideal)
}
```

### Estrategias para DONA:

```python
# 1. ROUTER PARA ELEGIR MODELO
def dona_router(query, intent):
    """
    Queries simples â†’ modelo rÃ¡pido
    Queries complejas â†’ modelo capaz
    """
    if intent in ["saludo", "consulta_precio_simple"]:
        return "gpt-3.5-turbo"  # RÃ¡pido: ~800ms
    elif intent in ["recomendacion", "calculo_materiales"]:
        return "gpt-4"  # Capaz: ~1500ms
    else:
        return "gpt-3.5-turbo"  # Default rÃ¡pido

# 2. CACHE DE RESPUESTAS FRECUENTES
DONA_CACHE = {
    "preguntas_frecuentes": [
        ("horario de atencion", "Estamos abiertos de..."),
        ("donde estan ubicados", "Nos encontrÃ¡s en..."),
        ("hacen envios", "SÃ­, hacemos envÃ­os a..."),
        ("formas de pago", "Aceptamos efectivo, tarjeta..."),
    ],
    
    # Para productos mÃ¡s consultados
    "productos_top": {
        "cemento_portland": "cached_response_with_current_price",
        "hierro_8": "cached_response_with_current_price",
    }
}

def check_cache(query):
    """
    Buscar match en cache antes de procesar
    """
    query_embedding = embed(query)
    
    for cached_query, cached_response in DONA_CACHE["preguntas_frecuentes"]:
        similarity = cosine_sim(query_embedding, embed(cached_query))
        if similarity > 0.92:  # Threshold alto
            return cached_response
    
    return None  # No cache hit, procesar normalmente
```

### OptimizaciÃ³n de componentes para DONA:

```python
# 3. EVALUAR CADA COMPONENTE

DONA_COMPONENT_EVALUATION = {
    "query_rewriting": {
        "latency_added": 80,  # ms
        "quality_improvement": "3% better relevancy",
        "decision": "KEEP - beneficio vale la latencia"
    },
    
    "re_ranking": {
        "latency_added": 120,  # ms
        "quality_improvement": "8% better relevancy",
        "decision": "KEEP - beneficio significativo"
    },
    
    "intent_detection": {
        "latency_added": 50,  # ms
        "quality_improvement": "Enables routing",
        "decision": "KEEP - habilita optimizaciones"
    }
}

# 4. STREAMING PARA MEJOR UX
def dona_stream_response(query):
    """
    Streaming hace que la latencia PERCIBIDA sea menor
    aunque la latencia total sea la misma.
    
    Usuario ve tokens aparecer â†’ se siente mÃ¡s rÃ¡pido
    """
    for token in llm.stream(prompt):
        yield token
```

### Targets de latencia para DONA:

```
TIPO DE QUERY          â”‚ TARGET    â”‚ ACTUAL    â”‚ STATUS
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€
Saludo                 â”‚ < 500ms   â”‚ 400ms     â”‚ âœ…
Precio simple          â”‚ < 1500ms  â”‚ 1200ms    â”‚ âœ…
Disponibilidad         â”‚ < 1500ms  â”‚ 1300ms    â”‚ âœ…
RecomendaciÃ³n          â”‚ < 3000ms  â”‚ 2500ms    â”‚ âœ…
CÃ¡lculo de materiales  â”‚ < 5000ms  â”‚ 4000ms    â”‚ âœ…
```

---

## Resumen del CapÃ­tulo 45

| Estrategia | Impacto | Dificultad |
|------------|---------|------------|
| **Modelo mÃ¡s pequeÃ±o** | Alto | Baja |
| **Router inteligente** | Alto | Media |
| **Caching** | Alto (para prompts repetidos) | Media |
| **Remover componentes** | Variable | Baja |
| **Binary quantization** | Medio | Baja |
| **Sharding** | Medio | Alta |

---

## Key Takeaways:

```
1. La latencia depende del CONTEXTO de tu aplicaciÃ³n
   (e-commerce vs diagnÃ³stico mÃ©dico)

2. CASI TODA la latencia viene de TRANSFORMERS
   â†’ El LLM es el MAYOR CULPABLE

3. ORDEN DE PRIORIDAD:
   1. Core LLM (modelos pequeÃ±os, routing, caching)
   2. Otros transformers (evaluar si valen la pena)
   3. Retrieval (binary quantization, sharding)

4. MEDIR latencia Y calidad de cada componente
   para tomar decisiones informadas

5. Con observabilidad robusta, podÃ©s iterar
   hasta alcanzar la latencia que tu proyecto necesita
```

---

## PrÃ³ximo: Multimodal RAG

Incorporando imÃ¡genes y PDFs en sistemas RAG.

---
