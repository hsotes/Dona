# Cap√≠tulo 33: Hallucinations y Grounding

---

## El Problema Central

> "Las hallucinations son una preocupaci√≥n constante cuando trabaj√°s con LLMs, e incluso un sistema RAG bien dise√±ado puede seguir alucinando."

---

## Ejemplo: El Descuento Inventado

```
ESCENARIO:
Chatbot de customer service para tienda online.

Usuario: "¬øTienen descuentos para estudiantes?"

Retriever encuentra:
‚îú‚îÄ‚îÄ Info sobre descuento para seniors (10%)
‚îî‚îÄ‚îÄ Info sobre descuento para clientes nuevos (10%)

System prompt: "S√© √∫til con los clientes"

LLM responde:
"¬°Absolutamente! Pod√©s obtener 10% de descuento con 
tu credencial de estudiante. El mismo descuento que 
ofrecemos a seniors y clientes nuevos."

EL PROBLEMA: El descuento de estudiantes NO EXISTE.
El LLM lo invent√≥.
```

---

## Por Qu√© los LLMs Alucinan

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                                                         ‚îÇ
‚îÇ   Los LLMs est√°n dise√±ados para producir               ‚îÇ
‚îÇ   SECUENCIAS DE TEXTO PROBABLES                        ‚îÇ
‚îÇ   con algo de randomizaci√≥n para variedad.             ‚îÇ
‚îÇ                                                         ‚îÇ
‚îÇ   Secuencias probables FRECUENTEMENTE son precisas     ‚îÇ
‚îÇ   PERO NO SIEMPRE.                                     ‚îÇ
‚îÇ                                                         ‚îÇ
‚îÇ   Los LLMs NO diferencian entre:                       ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ VERDADERO y FALSO                               ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ Solo entre PROBABLE e IMPROBABLE                ‚îÇ
‚îÇ                                                         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## Por Qu√© las Hallucinations son Problem√°ticas

```
1. INFORMACI√ìN INCORRECTA
   No quer√©s dar info falsa a usuarios.

2. SUENAN PLAUSIBLES
   Casi por definici√≥n, las alucinaciones suenan cre√≠bles.
   M√°s dif√≠cil de detectar que nonsense total.

3. EROSI√ìN DE CONFIANZA
   Con el tiempo, alucinaciones ocasionales causan
   que usuarios pierdan confianza en tu sistema,
   incluso si la mayor√≠a del contenido es preciso.
```

---

## Tipos de Hallucinations

### Nivel 1: Error de detalle

```
REALIDAD: Descuento senior es 10%
LLM DICE: Descuento senior es 5%

El concepto existe, pero el detalle est√° mal.
```

### Nivel 2: Negaci√≥n incorrecta

```
REALIDAD: Existe descuento senior
LLM DICE: No tenemos descuento para seniors

Niega algo que s√≠ existe.
```

### Nivel 3: Invenci√≥n completa

```
REALIDAD: No existe descuento estudiante
LLM DICE: Tenemos descuento estudiante del 10%

Inventa algo que no existe.
```

---

## La Verdad Fr√≠a y Dura

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                                                         ‚îÇ
‚îÇ   NO HAY SOLUCI√ìN PERFECTA PARA HALLUCINATIONS         ‚îÇ
‚îÇ                                                         ‚îÇ
‚îÇ   Al menos no actualmente.                             ‚îÇ
‚îÇ                                                         ‚îÇ
‚îÇ   PERO: RAG es uno de los mejores approaches           ‚îÇ
‚îÇ   disponibles actualmente, y hay formas de             ‚îÇ
‚îÇ   refinarlo para reducir a√∫n m√°s las alucinaciones.    ‚îÇ
‚îÇ                                                         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## T√©cnica 1: Self-Consistency Checking

### Sin knowledge base:

```
Hacer que el modelo genere M√öLTIPLES completions
para el mismo prompt y verificar si la info factual
es CONSISTENTE entre ellas.

IDEA: Si el LLM est√° alucinando, lo har√° inconsistentemente.
      Diferencias factuales ser√°n detectables.

PROBLEMA: Costoso y poco confiable en la pr√°ctica.
```

---

## T√©cnica 2: Grounding en Knowledge Base

### El approach principal para RAG:

```
Modificar el system prompt para que el LLM
SOLO haga claims factuales basados en info recuperada.
```

### Ejemplo de prompt:

```python
GROUNDED_SYSTEM_PROMPT = """
Sos un asistente que responde preguntas bas√°ndote 
√öNICAMENTE en los documentos proporcionados.

REGLAS ESTRICTAS:
1. Solo pod√©s hacer afirmaciones factuales que est√©n 
   DIRECTAMENTE soportadas por los documentos
2. Si la informaci√≥n no est√° en los documentos, dec√≠:
   "No tengo informaci√≥n sobre eso en los documentos disponibles"
3. NO infer√≠s, NO asum√≠s, NO complet√°s informaci√≥n faltante
4. Si no est√°s seguro, expres√° la incertidumbre

PROHIBIDO:
- Inventar informaci√≥n
- Generalizar m√°s all√° de lo que dicen los documentos
- Asumir que algo existe porque "tiene sentido"
"""
```

---

## T√©cnica 3: Citar Fuentes

### La idea:

```
Requerir que el LLM CITE las fuentes de cada claim.

BENEFICIOS:
‚îú‚îÄ‚îÄ Aumenta probabilidad de que use los documentos
‚îú‚îÄ‚îÄ Facilita verificaci√≥n por humanos
‚îî‚îÄ‚îÄ Hace las alucinaciones m√°s obvias
```

### Implementaci√≥n en prompt:

```python
CITATION_PROMPT = """
Al responder, cit√° la fuente de cada afirmaci√≥n factual.

Formato:
"El cemento Portland cuesta $8,500 [Documento 1]"
"El descuento para seniors es del 10% [Documento 3]"

Si no pod√©s citar una fuente para una afirmaci√≥n,
NO la incluyas en tu respuesta.
"""
```

### El riesgo:

```
‚ö†Ô∏è EL LLM PUEDE ALUCINAR LAS CITAS TAMBI√âN

"El descuento estudiante es 10% [Documento 2]"
‚Üí Pero el Documento 2 no dice nada de estudiantes

Necesit√°s verificaci√≥n externa para citas confiables.
```

---

## T√©cnica 4: ContextCite

### Sistema de verificaci√≥n externa:

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                                                         ‚îÇ
‚îÇ   ContextCite procesa la respuesta ORACI√ìN POR ORACI√ìN ‚îÇ
‚îÇ                                                         ‚îÇ
‚îÇ   Para cada oraci√≥n:                                   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ Atribuye a uno de los documentos de contexto    ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ Genera tag indicando la fuente                  ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ Si no hay soporte, marca "NO SOURCE"            ‚îÇ
‚îÇ                                                         ‚îÇ
‚îÇ   Algunas implementaciones dan similarity score        ‚îÇ
‚îÇ   entre la oraci√≥n y el documento fuente.              ‚îÇ
‚îÇ                                                         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Uso:

```
1. GENERAR CITAS:
   Tags se usan para agregar citas al output final.

2. EVALUACI√ìN:
   Medir qu√© tan frecuentemente el LLM se basa
   en documentos vs alucina.
```

---

## T√©cnica 5: ALCE Benchmark

### Para evaluar tu sistema:

```
ALCE (Automatic LLM Citation Evaluation):

1. Provee knowledge bases pre-armados
2. Provee sample questions
3. Tu sistema RAG procesa las preguntas
4. ALCE eval√∫a las respuestas generadas
```

### M√©tricas que eval√∫a:

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                                                         ‚îÇ
‚îÇ   FLUENCY:                                             ‚îÇ
‚îÇ   ¬øQu√© tan claro es el texto final?                   ‚îÇ
‚îÇ                                                         ‚îÇ
‚îÇ   CORRECTNESS:                                         ‚îÇ
‚îÇ   ¬øQu√© tan factualmente preciso?                      ‚îÇ
‚îÇ                                                         ‚îÇ
‚îÇ   CITATION QUALITY:                                    ‚îÇ
‚îÇ   ¬øQu√© tan bien las citas se alinean con las fuentes? ‚îÇ
‚îÇ                                                         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

> "Estos benchmarks no controlan hallucinations en producci√≥n, pero dan sentido de qu√© tan bien tu sistema las evita."

---

## Resumen de Estrategias

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                                                         ‚îÇ
‚îÇ   PASO 1: Construir RAG                                ‚îÇ
‚îÇ   Ya es el paso m√°s efectivo para minimizar            ‚îÇ
‚îÇ   hallucinations.                                      ‚îÇ
‚îÇ                                                         ‚îÇ
‚îÇ   PASO 2: Refinar System Prompt                        ‚îÇ
‚îÇ   Asegurar que el LLM base sus respuestas en          ‚îÇ
‚îÇ   informaci√≥n recuperada.                              ‚îÇ
‚îÇ                                                         ‚îÇ
‚îÇ   PASO 3: Requerir Citas                              ‚îÇ
‚îÇ   Forzar al LLM a citar fuentes para cada claim.      ‚îÇ
‚îÇ                                                         ‚îÇ
‚îÇ   PASO 4: Verificaci√≥n Externa                        ‚îÇ
‚îÇ   Usar sistemas como ContextCite para validar citas.  ‚îÇ
‚îÇ                                                         ‚îÇ
‚îÇ   PASO 5: Testear con Benchmarks                      ‚îÇ
‚îÇ   Evaluar sistema con ALCE u otros benchmarks.        ‚îÇ
‚îÇ                                                         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## Aplicaci√≥n para DONA üéØ

### System Prompt Anti-Hallucination:

```python
DONA_GROUNDED_PROMPT = """
Sos DONA, asistente de Materiales Boto Mariani.

REGLAS DE GROUNDING (MUY IMPORTANTES):
1. SOLO respond√©s con informaci√≥n de los documentos proporcionados
2. Si un producto no est√° en los documentos, dec√≠:
   "No encontr√© ese producto en nuestro cat√°logo actual. 
    ¬øQuer√©s que busque algo similar?"
3. NUNCA invent√©s precios, disponibilidad, o caracter√≠sticas
4. Si no est√°s segura de algo, pregunt√° al cliente
5. Cit√° el producto espec√≠fico cuando des informaci√≥n

EJEMPLOS DE QU√â NO HACER:
‚ùå "Seguramente tenemos eso" (si no est√° en docs)
‚ùå "El precio debe ser alrededor de..." (si no hay precio exacto)
‚ùå "Todos nuestros productos tienen garant√≠a" (si no est√° en docs)

EJEMPLOS DE QU√â HACER:
‚úÖ "Seg√∫n el cat√°logo, el cemento Portland est√° a $8,500 [Producto 1]"
‚úÖ "No veo hierro del 12 en los productos encontrados. 
    Tenemos del 8 y del 10, ¬øte sirve alguno?"
‚úÖ "No tengo informaci√≥n sobre garant√≠a en estos documentos. 
    ¬øQuer√©s que consulte con un vendedor?"
"""
```

### Detecci√≥n de Hallucinations para DONA:

```python
def check_grounding(response, retrieved_docs):
    """
    Verificar si la respuesta est√° grounded en los documentos.
    Retorna warnings si detecta posibles hallucinations.
    """
    warnings = []
    
    # Extraer precios mencionados en la respuesta
    precios_respuesta = extract_prices(response)
    precios_docs = [doc['precio'] for doc in retrieved_docs]
    
    for precio in precios_respuesta:
        if precio not in precios_docs:
            warnings.append(f"‚ö†Ô∏è Precio ${precio} no encontrado en documentos")
    
    # Verificar productos mencionados
    productos_respuesta = extract_product_names(response)
    productos_docs = [doc['nombre'] for doc in retrieved_docs]
    
    for producto in productos_respuesta:
        if not any(prod in producto for prod in productos_docs):
            warnings.append(f"‚ö†Ô∏è Producto '{producto}' no encontrado en documentos")
    
    # Detectar frases de riesgo
    risky_phrases = [
        "seguramente tenemos",
        "probablemente",
        "debe costar",
        "aproximadamente",
        "todos nuestros productos"
    ]
    
    for phrase in risky_phrases:
        if phrase.lower() in response.lower():
            warnings.append(f"‚ö†Ô∏è Frase de riesgo detectada: '{phrase}'")
    
    return warnings
```

### Logging para detectar patrones:

```python
def log_response_quality(query, response, docs, warnings):
    """Loggear para an√°lisis posterior"""
    log_entry = {
        "timestamp": datetime.now(),
        "query": query,
        "response": response,
        "num_docs": len(docs),
        "grounding_warnings": warnings,
        "is_grounded": len(warnings) == 0
    }
    
    # Guardar para an√°lisis
    save_to_analytics(log_entry)
    
    # Alertar si hay muchos warnings
    if len(warnings) >= 2:
        alert_for_review(log_entry)
```

---

## Resumen del Cap√≠tulo 33

| Estrategia | Qu√© hace | Efectividad |
|------------|----------|-------------|
| **RAG mismo** | Provee info para grounding | Alta (base) |
| **System Prompt** | Instruye a usar solo docs | Media-Alta |
| **Citar Fuentes** | Fuerza atribuci√≥n | Media |
| **ContextCite** | Verifica citas externamente | Alta |
| **ALCE Benchmark** | Eval√∫a sistema | Para testing |

---

## Key Takeaways:

```
1. NO HAY SOLUCI√ìN PERFECTA para hallucinations

2. RAG es el paso M√ÅS EFECTIVO para minimizarlas

3. Refinar system prompt para GROUNDING es el siguiente paso

4. Requerir CITAS ayuda pero el LLM puede alucinar citas tambi√©n

5. Verificaci√≥n EXTERNA (ContextCite) da m√°s confianza

6. TESTEAR con benchmarks para medir qu√© tan bien funciona
```

---

## Pr√≥ximo: Evaluaci√≥n de LLMs en RAG

M√©tricas y benchmarks espec√≠ficos para evaluar el componente LLM.

---
