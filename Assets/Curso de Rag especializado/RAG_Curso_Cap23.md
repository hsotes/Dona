# Cap√≠tulo 23: Query Parsing

---

## El Problema

> "Los sistemas RAG a menudo se despliegan en contextos donde el usuario espera interactuar con un LLM de manera conversacional, como si estuviera chateando con otra persona."

### Resultado:

```
Prompts escritos por humanos = MALAS queries de b√∫squeda

En lugar de enviar esos prompts directamente a la Vector DB,
el retriever puede PARSEAR el prompt para:
‚îú‚îÄ‚îÄ Identificar su intenci√≥n
‚îú‚îÄ‚îÄ Editar
‚îú‚îÄ‚îÄ Reescribir
‚îî‚îÄ‚îÄ Transformar completamente
```

---

## T√©cnica 1: Query Rewriting (La M√°s Usada)

### La idea:

```
Usar un LLM para REESCRIBIR la query
antes de enviarla al retriever.
```

### Ejemplo de prompt para el rewriter:

```
"El siguiente prompt fue enviado por un usuario para 
consultar una base de datos de documentos m√©dicos 
que vinculan s√≠ntomas con diagn√≥sticos.

Reescrib√≠ el prompt para optimizarlo para b√∫squeda:
- Clarific√° frases ambiguas
- Us√° terminolog√≠a m√©dica donde aplique
- Agreg√° sin√≥nimos que aumenten chances de match
- Remov√© informaci√≥n innecesaria o distractora

[PROMPT DEL USUARIO]"
```

---

### Ejemplo Real:

#### Query original del usuario:

```
"I was out walking my dog, a beautiful black lab named Poppy, 
when she raced away from me and yanked on her leash hard 
while I was holding it. Three days later, my shoulder is 
still numb and my fingers are all pins and needles. 
What's going on?"
```

#### Problemas de esta query:

```
‚ùå Info irrelevante: nombre del perro, raza, que es "beautiful"
‚ùå Lenguaje coloquial: "yanked", "pins and needles"
‚ùå No usa terminolog√≠a m√©dica
‚ùå Dif√≠cil de matchear con documentos m√©dicos t√©cnicos
```

#### Query reescrita por LLM:

```
"Experienced a sudden forceful pull on the shoulder, 
resulting in persistent shoulder numbness and finger 
numbness for three days. What are the potential causes 
or diagnoses such as neuropathy or nerve impingement?"
```

#### Mejoras:

```
‚úÖ Removi√≥ info irrelevante (perro, nombre, paseo)
‚úÖ Clarific√≥ la ambig√ºedad
‚úÖ Us√≥ terminolog√≠a m√©dica (neuropathy, nerve impingement)
‚úÖ Estructura clara de s√≠ntomas
```

---

### Vale la pena el costo adicional:

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                                                         ‚îÇ
‚îÇ   Query Rewriting agrega:                              ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ Una llamada extra al LLM                         ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ Algo de latencia                                 ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ Costo de API                                     ‚îÇ
‚îÇ                                                         ‚îÇ
‚îÇ   PERO los beneficios son SUSTANCIALES:                ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ Mucho mejor matching en retrieval                ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ Menos ruido en los resultados                    ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ LLM final recibe mejor contexto                  ‚îÇ
‚îÇ                                                         ‚îÇ
‚îÇ   En general, los beneficios JUSTIFICAN f√°cilmente     ‚îÇ
‚îÇ   el costo adicional.                                  ‚îÇ
‚îÇ                                                         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## T√©cnica 2: Named Entity Recognition (NER)

### La idea:

```
Reconocer CATEGOR√çAS de informaci√≥n en la query:
‚îú‚îÄ‚îÄ Lugares
‚îú‚îÄ‚îÄ Personas
‚îú‚îÄ‚îÄ Fechas
‚îú‚îÄ‚îÄ Personajes ficticios
‚îú‚îÄ‚îÄ Productos
‚îî‚îÄ‚îÄ etc.
```

### Uso:

```
La info extra√≠da puede usarse para:
‚îú‚îÄ‚îÄ Informar el vector search
‚îî‚îÄ‚îÄ Configurar metadata filtering
```

### Ejemplo con GLiNER:

```python
from gliner import GLiNER

model = GLiNER.load("gliner_base")

text = "John Smith visited New York on January 15th 
        to meet Harry Potter author J.K. Rowling"

labels = ["person", "location", "date", "character", "book"]

entities = model.predict(text, labels)

# Resultado:
# [
#   {"text": "John Smith", "label": "person"},
#   {"text": "New York", "label": "location"},
#   {"text": "January 15th", "label": "date"},
#   {"text": "Harry Potter", "label": "character"},
#   {"text": "J.K. Rowling", "label": "person"}
# ]
```

### Beneficios:

```
‚úÖ Modelo muy eficiente (puede correr en cada query)
‚úÖ Mejora significativa en calidad de retrieval
‚ùå Agrega algo de latencia
```

---

## T√©cnica 3: HyDE (Hypothetical Document Embeddings)

### La idea:

```
Generar un DOCUMENTO HIPOT√âTICO que ser√≠a
el resultado IDEAL de la b√∫squeda.

Luego usar ESE documento para hacer la b√∫squeda.
```

### El problema que resuelve:

```
NORMALMENTE:
Query (pregunta) ‚Üí Buscar ‚Üí Documentos (respuestas)

El retriever intenta matchear TIPOS DIFERENTES de texto:
‚îú‚îÄ‚îÄ Query: "¬øQu√© causa entumecimiento en el hombro?"
‚îî‚îÄ‚îÄ Documento: "El pinzamiento nervioso puede causar..."

Es como comparar manzanas con naranjas.
```

### Con HyDE:

```
PASO 1: Query ‚Üí LLM genera documento hipot√©tico

Query: "¬øQu√© causa entumecimiento en el hombro?"

Documento hipot√©tico generado:
"El entumecimiento del hombro puede ser causado por 
diversas condiciones. El pinzamiento nervioso ocurre 
cuando un nervio es comprimido, causando entumecimiento 
y hormigueo. La neuropat√≠a tambi√©n puede producir estos 
s√≠ntomas. Condiciones como el s√≠ndrome del t√∫nel carpiano..."

PASO 2: Embeber el documento hipot√©tico
        (no la query original)

PASO 3: Buscar documentos similares al hipot√©tico
```

### Por qu√© funciona:

```
Ahora compar√°s:
‚îú‚îÄ‚îÄ Documento hipot√©tico (generado)
‚îî‚îÄ‚îÄ Documentos reales (knowledge base)

Comparando naranjas con naranjas.
El retriever entiende no solo la INTENCI√ìN
sino tambi√©n c√≥mo LUCE un resultado de calidad.
```

### Trade-off:

```
‚úÖ Mejoras reales en performance
‚ùå Latencia adicional
‚ùå Costo computacional del LLM
```

---

## Comparaci√≥n de T√©cnicas

| T√©cnica | Complejidad | Beneficio | Cu√°ndo usar |
|---------|-------------|-----------|-------------|
| **Query Rewriting** | Baja | Alto | SIEMPRE (default) |
| **NER** | Media | Medio | Cuando metadata filtering importa |
| **HyDE** | Alta | Alto | Cuando precision es cr√≠tica |

---

## Recomendaci√≥n

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                                                         ‚îÇ
‚îÇ   EN CASI TODOS LOS CASOS:                             ‚îÇ
‚îÇ                                                         ‚îÇ
‚îÇ   Basic Query Rewriting es suficiente.                 ‚îÇ
‚îÇ                                                         ‚îÇ
‚îÇ   Un LLM bien prompteado haciendo "touch-ups" b√°sicos  ‚îÇ
‚îÇ   en los prompts del usuario es el approach correcto.  ‚îÇ
‚îÇ                                                         ‚îÇ
‚îÇ   T√©cnicas avanzadas (NER, HyDE):                      ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ Pueden dar beneficios adicionales                ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ M√°s complejas de implementar                     ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ No necesariamente dan mejores resultados         ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ Experiment√° y dej√° que los resultados decidan    ‚îÇ
‚îÇ                                                         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## Aplicaci√≥n para DONA üéØ

### Query Rewriting para DONA:

```python
DONA_QUERY_REWRITER_PROMPT = """
El siguiente mensaje fue enviado por un cliente para 
consultar el cat√°logo de materiales de construcci√≥n.

Reescrib√≠ la consulta para optimizarla:
- Identific√° el producto o tipo de producto buscado
- Us√° nombres t√©cnicos correctos de materiales
- Agreg√° sin√≥nimos comunes (fierro/hierro, cemento/portland)
- Extra√© especificaciones (medidas, marcas, cantidades)
- Remov√© saludos y texto conversacional

Query del usuario: {user_query}
"""
```

### Ejemplo para DONA:

```
QUERY ORIGINAL:
"Hola che, necesito algo para armar unas columnas, 
el fierro ese que viene en barras largas, creo que 
del 8 o del 10, no me acuerdo bien. Ah y tambi√©n 
necesitar√≠a estribos."

QUERY REESCRITA:
"Hierro/varilla de construcci√≥n para columnas, 
di√°metro 8mm o 10mm. Estribos para armado de columnas."
```

### NER para DONA:

```python
# Entidades √∫tiles para DONA
labels = ["producto", "medida", "marca", "cantidad", "uso"]

query = "Necesito 10 bolsas de cemento Loma Negra para la losa"

# Resultado:
# [
#   {"text": "10 bolsas", "label": "cantidad"},
#   {"text": "cemento", "label": "producto"},
#   {"text": "Loma Negra", "label": "marca"},
#   {"text": "losa", "label": "uso"}
# ]

# Esto puede informar metadata filtering:
# categoria="cementos", marca="Loma Negra"
```

### Flujo recomendado para DONA:

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                                                         ‚îÇ
‚îÇ   1. Usuario env√≠a mensaje conversacional              ‚îÇ
‚îÇ      "Che, cu√°nto sale el fierro del 8?"               ‚îÇ
‚îÇ                                                         ‚îÇ
‚îÇ   2. Query Rewriting (LLM)                             ‚îÇ
‚îÇ      "Precio hierro/varilla construcci√≥n 8mm"          ‚îÇ
‚îÇ                                                         ‚îÇ
‚îÇ   3. (Opcional) NER                                    ‚îÇ
‚îÇ      producto: "hierro", medida: "8mm"                 ‚îÇ
‚îÇ                                                         ‚îÇ
‚îÇ   4. Hybrid Search con metadata filter                 ‚îÇ
‚îÇ      categoria="hierros", activo=True                  ‚îÇ
‚îÇ                                                         ‚îÇ
‚îÇ   5. Resultados al LLM para respuesta final            ‚îÇ
‚îÇ                                                         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## Resumen del Cap√≠tulo 23

| T√©cnica | Qu√© hace | Recomendaci√≥n |
|---------|----------|---------------|
| **Query Rewriting** | LLM limpia y mejora la query | USAR SIEMPRE |
| **NER** | Identifica entidades (productos, fechas, etc.) | Opcional, √∫til para filtering |
| **HyDE** | Genera documento hipot√©tico ideal | Solo si precision es cr√≠tica |

---

## Key Takeaway:

> "Tener alg√∫n tipo de query parsing es una pieza CLAVE de tu sistema RAG. En casi todos los casos, basic query rewriting es el approach correcto."

---

## Pr√≥ximo: Re-ranking

C√≥mo mejorar el ranking de resultados despu√©s del retrieval inicial.

---
