# Cap√≠tulo 25: Re-ranking

---

## ¬øQu√© es Re-ranking?

> "Re-ranking es un proceso post-retrieval donde el conjunto inicial de documentos retornados por la vector database son re-rankeados usando modelos de alto rendimiento pero costosos, para asegurar que se retornen los documentos absolutamente m√°s relevantes."

---

## La Idea Principal

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                                                         ‚îÇ
‚îÇ   Re-ranking mejora la CALIDAD del retrieval:          ‚îÇ
‚îÇ                                                         ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ DESPU√âS de que la Vector DB retorna resultados   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ ANTES de que se env√≠en al LLM                   ‚îÇ
‚îÇ                                                         ‚îÇ
‚îÇ   Re-scoring y re-ranking usando modelos m√°s capaces   ‚îÇ
‚îÇ   que son costosos pero solo procesan pocos docs.      ‚îÇ
‚îÇ                                                         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## Por Qu√© Funciona

### El problema sin re-ranking:

```
Prompt: "What is the capital of Canada?"

Vector search retorna docs sem√°nticamente relacionados
pero que NO responden la pregunta:

1. "Toronto is in Canada"              ‚Üê relacionado pero no responde
2. "The capital of France is Paris"    ‚Üê tiene "capital" pero no responde
3. "Canada is the maple syrup capital" ‚Üê tiene ambas palabras pero no responde
4. "Ottawa is the capital of Canada"   ‚Üê ‚úÖ RESPONDE
```

### Con re-ranking:

```
Re-ranker analiza cada doc contra el prompt:

1. "Ottawa is the capital of Canada"   ‚Üí Score: 0.95 ‚Üê TOP
2. "Toronto is in Canada"              ‚Üí Score: 0.30
3. "The capital of France is Paris"    ‚Üí Score: 0.20
4. "Canada is maple syrup capital"     ‚Üí Score: 0.15

Resultado: El documento correcto sube al top.
```

---

## El Flujo de Re-ranking

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                                                         ‚îÇ
‚îÇ   1. OVER-FETCH con Vector DB                          ‚îÇ
‚îÇ      Hybrid search ‚Üí 20-100 documentos                 ‚îÇ
‚îÇ      (m√°s de los que realmente necesit√°s)              ‚îÇ
‚îÇ                                                         ‚îÇ
‚îÇ   2. RE-RANK                                           ‚îÇ
‚îÇ      Cross-encoder re-scorea cada documento            ‚îÇ
‚îÇ      Genera nuevo ranking basado en scores             ‚îÇ
‚îÇ                                                         ‚îÇ
‚îÇ   3. RETURN SUBSET                                     ‚îÇ
‚îÇ      Solo los top 5-10 documentos                      ‚îÇ
‚îÇ      (mucho m√°s relevantes que sin re-ranking)         ‚îÇ
‚îÇ                                                         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Visualizaci√≥n:

```
PASO 1: Vector DB retrieval (over-fetch)

Prompt ‚Üí Hybrid Search ‚Üí [doc1, doc2, doc3, ... doc50]
                         (50 docs, algunos buenos, algunos no tanto)

PASO 2: Re-ranking con Cross-encoder

[Prompt + doc1] ‚Üí Cross-encoder ‚Üí 0.45
[Prompt + doc2] ‚Üí Cross-encoder ‚Üí 0.92 ‚Üê muy relevante
[Prompt + doc3] ‚Üí Cross-encoder ‚Üí 0.12
...
[Prompt + doc50] ‚Üí Cross-encoder ‚Üí 0.33

PASO 3: Nuevo ranking y subset

Ordenar por score ‚Üí [doc2, doc17, doc31, doc5, doc42, ...]
Retornar top 5 ‚Üí [doc2, doc17, doc31, doc5, doc42]

Estos 5 son MUCHO m√°s relevantes que los top 5 originales.
```

---

## Por Qu√© Cross-Encoder Funciona Aqu√≠

### Recordatorio del problema:

```
Cross-encoder: Excelente calidad, pero MUY LENTO.
Infeasible con millones de documentos.
```

### La soluci√≥n: Usarlo DESPU√âS del filtro inicial

```
ANTES (imposible):
1 mill√≥n de docs √ó cross-encoder = 1 mill√≥n de operaciones ‚ùå

DESPU√âS (posible):
1 mill√≥n de docs ‚Üí Vector DB ‚Üí 50 docs ‚Üí cross-encoder = 50 operaciones ‚úÖ

El bi-encoder ya filtr√≥ el 99.995% de los documentos.
Ahora el cross-encoder solo procesa los 50 candidatos.
```

### El trade-off:

```
‚úÖ Mucha mejor calidad de resultados
‚ùå Algo de latencia adicional

PERO: Este trade-off casi SIEMPRE vale la pena.
```

---

## LLM-Based Re-ranking

### La idea:

```
En lugar de cross-encoder, usar un LLM directamente.

[Prompt + Documento] ‚Üí LLM ‚Üí Score de relevancia

LLMs espec√≠ficamente dise√±ados para esta tarea pueden:
‚îú‚îÄ‚îÄ Analizar el par prompt-documento
‚îú‚îÄ‚îÄ Evaluar relevancia
‚îî‚îÄ‚îÄ Responder con un score num√©rico
```

### Trade-off:

```
‚úÖ Promisorio, puede ser muy preciso
‚ùå Tan ineficiente como cross-encoder
‚ùå Scoring no puede empezar hasta recibir el prompt
‚ùå Scorear un documento individual es costoso

Resultado: Tambi√©n es solo para RE-RANKING,
           no para b√∫squeda principal.
```

---

## Implementaci√≥n Pr√°ctica

### En muchas Vector DBs es muy simple:

```python
# Sin re-ranking
results = collection.query.hybrid(
    query="What is the capital of Canada?",
    limit=5
)

# CON re-ranking (una l√≠nea extra)
results = collection.query.hybrid(
    query="What is the capital of Canada?",
    limit=5,
    rerank=Rerank.cohere()  # o Rerank.cross_encoder()
)
```

### Configuraci√≥n t√≠pica:

```python
# Over-fetch y re-rank
results = collection.query.hybrid(
    query=user_query,
    limit=5,                    # Queremos 5 docs finales
    auto_limit=25,              # Over-fetch 25 docs
    rerank=Rerank.cross_encoder(
        model="cross-encoder/ms-marco-MiniLM-L-6-v2"
    )
)

# El sistema:
# 1. Trae 25 docs con hybrid search
# 2. Re-rankea los 25 con cross-encoder
# 3. Retorna los top 5
```

---

## Configuraci√≥n Recomendada

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                                                         ‚îÇ
‚îÇ   T√çPICO Y EFECTIVO:                                   ‚îÇ
‚îÇ                                                         ‚îÇ
‚îÇ   Over-fetch: 15-25 documentos                         ‚îÇ
‚îÇ   Re-rank entre ellos                                  ‚îÇ
‚îÇ   Retornar: 5-10 documentos                            ‚îÇ
‚îÇ                                                         ‚îÇ
‚îÇ   Resultado:                                           ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ Gran boost en relevancia                         ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ Poca latencia adicional                          ‚îÇ
‚îÇ                                                         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## Cu√°ndo Usar Re-ranking

```
‚úÖ SIEMPRE que la relevancia de b√∫squeda sea importante
   (casi siempre en RAG)

‚úÖ Una de las PRIMERAS t√©cnicas a explorar
   cuando quer√©s mejorar tu pipeline

‚úÖ F√°cil de implementar
   (a menudo una sola l√≠nea de c√≥digo)

‚ùå Si la latencia adicional es inaceptable
   (pero t√≠picamente es muy poca)
```

---

## Aplicaci√≥n para DONA üéØ

### Setup de re-ranking para DONA:

```python
# B√∫squeda de productos con re-ranking
def buscar_producto(query: str):
    # 1. Query rewriting
    query_limpia = rewrite_query(query)
    
    # 2. Hybrid search con over-fetch
    results = collection.query.hybrid(
        query=query_limpia,
        alpha=0.5,
        limit=5,          # Queremos 5 productos
        auto_limit=20,    # Over-fetch 20
        rerank=Rerank.cross_encoder(),
        filters=Filter.by_property("disponibilidad").equal("en_stock")
    )
    
    return results
```

### Ejemplo de mejora con re-ranking:

```
Query: "cemento para hacer una vereda"

SIN RE-RANKING (top 5 de hybrid search):
1. Cemento de contacto (tiene "cemento" pero no aplica)
2. Cemento blanco (relacionado pero no ideal)
3. Cemento Portland (‚úÖ correcto)
4. Arena fina (relacionado a veredas)
5. Cemento r√°pido (relacionado pero espec√≠fico)

CON RE-RANKING:
1. Cemento Portland (‚úÖ ideal para veredas)
2. Cemento Portland tipo I (‚úÖ tambi√©n correcto)
3. Arena fina (necesaria para mezcla)
4. Cemento r√°pido (alternativa)
5. Cal hidratada (se usa en mezclas)

El re-ranker entiende que "para hacer vereda" implica
cemento de construcci√≥n, no cemento de contacto.
```

### M√©tricas esperadas:

```
SIN re-ranking:
‚îú‚îÄ‚îÄ Recall@5: 60%
‚îú‚îÄ‚îÄ Precision@5: 40%
‚îî‚îÄ‚îÄ MRR: 0.5

CON re-ranking:
‚îú‚îÄ‚îÄ Recall@5: 80%
‚îú‚îÄ‚îÄ Precision@5: 70%
‚îî‚îÄ‚îÄ MRR: 0.85

(N√∫meros ilustrativos - medir en tu sistema)
```

---

## Resumen del Cap√≠tulo 25

| Concepto | Explicaci√≥n |
|----------|-------------|
| **Re-ranking** | Proceso post-retrieval para mejorar relevancia |
| **Over-fetch** | Traer m√°s docs de los necesarios (20-100) |
| **Cross-encoder** | Modelo t√≠pico para re-ranking |
| **LLM re-ranking** | Alternativa usando LLM directamente |
| **Configuraci√≥n t√≠pica** | Over-fetch 15-25, retornar 5-10 |

---

## Key Takeaway:

> "Re-ranking es una de las PRIMERAS t√©cnicas que deber√≠as explorar para mejorar la relevancia de b√∫squeda. T√≠picamente, pod√©s over-fetch 15-25 documentos y re-rankear entre ellos para un gran boost en relevancia a costa de poca latencia adicional."

---

## Pr√≥ximo: Wrap-up M√≥dulo 3

Resumen de Vector Databases y t√©cnicas de producci√≥n.

---
