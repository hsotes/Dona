# CapÃ­tulo 5: CÃ³mo Funcionan los LLMs (Deep Dive)

---

## LLMs = "Fancy Autocomplete"

> "Todos los LLMs hacen es predecir la siguiente palabra que deberÃ­a aparecer en un texto."

### Ejemplo:

```
Prompt: "What a beautiful day, the sun is..."

Completions posibles:
â”œâ”€â”€ "shining" â† mÃ¡s probable
â”œâ”€â”€ "rising"  â† probable
â”œâ”€â”€ "out"     â† probable
â””â”€â”€ "exploding" â† improbable (pero vÃ¡lido gramaticalmente)
```

**Â¿Por quÃ© "exploding" es incorrecto?**
No es por gramÃ¡tica (es inglÃ©s vÃ¡lido), sino porque es **improbable**. El sol usualmente no explota, especialmente en un dÃ­a hermoso.

---

## CÃ³mo Genera Texto un LLM

### Paso a paso:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                         â”‚
â”‚  1. PROCESA el estado actual del texto                 â”‚
â”‚     â†’ Entiende relaciones entre palabras               â”‚
â”‚     â†’ Comprende significado general                    â”‚
â”‚                                                         â”‚
â”‚  2. CALCULA probabilidad para CADA token               â”‚
â”‚     â†’ Vocabulario: 10,000 - 100,000+ tokens            â”‚
â”‚     â†’ Cada token tiene una probabilidad                â”‚
â”‚                                                         â”‚
â”‚  3. ELIGE aleatoriamente de esa distribuciÃ³n           â”‚
â”‚     â†’ "shining": 80% de las veces                      â”‚
â”‚     â†’ "rising": 15% de las veces                       â”‚
â”‚     â†’ "exploding": 0.01% de las veces                  â”‚
â”‚                                                         â”‚
â”‚  4. REPITE para el siguiente token                     â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Tokens vs Palabras

> "TÃ©cnicamente, un LLM no genera palabras sino tokens - piezas de palabras."

### Ejemplos:

| Palabra | Tokens |
|---------|--------|
| "London" | 1 token |
| "door" | 1 token |
| "programmatically" | mÃºltiples tokens |
| "unhappy" | mÃºltiples tokens ("un" + "happy") |
| "." (punto) | 1 token |
| "?" | 1 token |

### Â¿Por quÃ© tokens?

```
Ventaja: Puede construir CUALQUIER palabra
         sin necesitar un token para cada una.

"programmatically" = "program" + "mat" + "ically"
```

---

## Comportamiento Autoregresivo

> "Autoregresivo = auto-influenciante. Las elecciones anteriores impactan las posteriores."

### Ejemplo:

```
Prompt: "The sun is..."

CAMINO A:
â”œâ”€â”€ Elige: "shining"
â”œâ”€â”€ Luego: "in"
â”œâ”€â”€ Luego: "the"
â””â”€â”€ Luego: "sky"
â†’ "The sun is shining in the sky"

CAMINO B:
â”œâ”€â”€ Elige: "warming"
â”œâ”€â”€ Luego: "our"
â””â”€â”€ Luego: "faces"
â†’ "The sun is warming our faces"
```

### ImplicaciÃ³n importante:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                         â”‚
â”‚   Una vez que el LLM elige una direcciÃ³n,              â”‚
â”‚   SIGUE ese camino donde lo lleve.                     â”‚
â”‚                                                         â”‚
â”‚   Por eso: mismo prompt â†’ diferentes respuestas        â”‚
â”‚   (randomness + autoregressive)                        â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## CÃ³mo se Entrena un LLM

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                         â”‚
â”‚   ANTES del training:                                  â”‚
â”‚   â”œâ”€â”€ Modelo con billones de parÃ¡metros               â”‚
â”‚   â””â”€â”€ Produce GIBBERISH (basura)                      â”‚
â”‚                                                         â”‚
â”‚   DURANTE el training:                                 â”‚
â”‚   â”œâ”€â”€ Se le muestran textos incompletos               â”‚
â”‚   â”œâ”€â”€ Intenta predecir la siguiente palabra           â”‚
â”‚   â”œâ”€â”€ Se corrigen sus parÃ¡metros segÃºn precisiÃ³n      â”‚
â”‚   â””â”€â”€ Repite TRILLONES de veces                       â”‚
â”‚                                                         â”‚
â”‚   DESPUÃ‰S del training:                                â”‚
â”‚   â”œâ”€â”€ AprendiÃ³ informaciÃ³n factual                    â”‚
â”‚   â”œâ”€â”€ AprendiÃ³ estilos lingÃ¼Ã­sticos                   â”‚
â”‚   â””â”€â”€ Puede generar texto coherente                   â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Datos de entrenamiento tÃ­picos:

```
â”œâ”€â”€ Trillones de tokens
â”œâ”€â”€ Mayormente de internet pÃºblico
â”œâ”€â”€ Variedad de estilos y temas
â””â”€â”€ Todo lo que estaba en el training data
```

---

## Por QuÃ© los LLMs Alucinan

> "LLMs estÃ¡n diseÃ±ados para generar texto PROBABLE, no texto VERDADERO."

### La definiciÃ³n de "verdad" para un LLM:

```
VERDAD HUMANA:          VERDAD LLM:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€        â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Corresponde a           Es probabilÃ­sticamente
la realidad             probable segÃºn el
                        training data
```

### CuÃ¡ndo alucinan:

| SituaciÃ³n | Por quÃ© alucina |
|-----------|-----------------|
| Datos privados de tu empresa | No estaban en el training |
| Noticias de hoy | Entrenado antes de hoy |
| Info especializada rara | Pocas menciones en training |
| Tu catÃ¡logo de productos | Definitivamente no lo vio |

> "El LLM no estÃ¡ teniendo un episodio psicolÃ³gico. EstÃ¡ haciendo exactamente lo que fue diseÃ±ado para hacer: generar texto probable."

---

## CÃ³mo RAG Soluciona Esto

### El insight clave:

> "Los LLMs son MUY buenos entendiendo contexto en el prompt, incluso si esa informaciÃ³n NO estaba en el training."

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                         â”‚
â”‚   SIN RAG:                                             â”‚
â”‚   Prompt: "Â¿CuÃ¡nto sale el cemento?"                   â”‚
â”‚   LLM: [busca en training data]                        â”‚
â”‚        â†’ No tiene tu catÃ¡logo                          â”‚
â”‚        â†’ ALUCINA un precio                             â”‚
â”‚                                                         â”‚
â”‚   CON RAG:                                             â”‚
â”‚   Prompt: "Contexto: Cemento Portland $8500            â”‚
â”‚            Pregunta: Â¿CuÃ¡nto sale el cemento?"         â”‚
â”‚   LLM: [lee el contexto en el prompt]                  â”‚
â”‚        â†’ TIENE la informaciÃ³n                          â”‚
â”‚        â†’ Responde correctamente                        â”‚
â”‚                                                         â”‚
â”‚   "La informaciÃ³n FUNDAMENTA (grounds) las respuestas" â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Limitaciones del Contexto

### 1. Costo computacional:

```
Antes de generar CADA token, el LLM escanea 
TODOS los tokens del prompt + completion.

Prompt mÃ¡s largo = mÃ¡s cÃ³mputo = mÃ¡s caro = mÃ¡s lento
```

### 2. Context Window:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                         â”‚
â”‚   CONTEXT WINDOW = mÃ¡ximo de tokens que puede procesar â”‚
â”‚                                                         â”‚
â”‚   Modelos viejos: ~4,000 tokens                        â”‚
â”‚   Modelos nuevos: hasta millones de tokens             â”‚
â”‚                                                         â”‚
â”‚   Si excedÃ©s el context window â†’ ERROR                 â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### El trade-off:

```
MÃS contexto:               MENOS contexto:
â”œâ”€â”€ MÃ¡s informaciÃ³n         â”œâ”€â”€ MÃ¡s barato
â”œâ”€â”€ Mejores respuestas      â”œâ”€â”€ MÃ¡s rÃ¡pido
â”œâ”€â”€ MÃ¡s caro                â”œâ”€â”€ Puede faltar info
â””â”€â”€ MÃ¡s lento               â””â”€â”€ Posibles alucinaciones
```

---

## Resumen: Por QuÃ© Importa para RAG

| CaracterÃ­stica LLM | ImplicaciÃ³n para RAG |
|--------------------|----------------------|
| Predice tokens probables | Puede "inventar" si no tiene data |
| Entiende contexto muy bien | Podemos DARLE info en el prompt |
| Autoregresivo | Una vez que toma direcciÃ³n, la sigue |
| Context window limitado | No podemos meter TODO, hay que elegir |
| MÃ¡s contexto = mÃ¡s caro | Retriever debe ser SELECTIVO |

---

## El Rol del Retriever (adelanto)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                         â”‚
â”‚   PROBLEMA:                                            â”‚
â”‚   - LLM necesita contexto para no alucinar            â”‚
â”‚   - Pero no podemos meter TODO en el prompt           â”‚
â”‚   - Context window limitado + costo                   â”‚
â”‚                                                         â”‚
â”‚   SOLUCIÃ“N: RETRIEVER                                  â”‚
â”‚   - Filtra la informaciÃ³n                             â”‚
â”‚   - Encuentra lo MÃS RELEVANTE                        â”‚
â”‚   - Presenta solo lo necesario                        â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## AplicaciÃ³n para DONA ğŸ¯

### Por quÃ© DONA puede estar fallando:

| Problema | Causa probable |
|----------|----------------|
| Inventa precios | No tiene el dato en contexto |
| Respuestas genÃ©ricas | Retriever no trae docs correctos |
| Mezcla productos | Demasiado contexto confuso |
| Respuestas inconsistentes | Autoregressive + random |

### Soluciones:

```
1. Asegurar que el Retriever traiga docs CORRECTOS
2. Prompt que FUERCE usar solo el contexto
3. No meter demasiado contexto (confunde)
4. Temperatura baja para menos randomness
```

---

## PrÃ³ximo: El Retriever

CÃ³mo el retriever encuentra y selecciona informaciÃ³n relevante.

---
