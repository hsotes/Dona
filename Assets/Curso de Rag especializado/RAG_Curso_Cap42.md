# CapÃ­tulo 42: Construyendo Custom Datasets

---

## El Valor de Custom Datasets

> "Crear un dataset custom de prompts que tu sistema recibiÃ³ te permite entender profundamente cÃ³mo tu sistema performÃ³ en el pasado, y correr experimentos para ver cÃ³mo un rediseÃ±o podrÃ­a cambiar la performance en prompts del mundo real."

---

## Â¿QuÃ© es un Custom Dataset?

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                         â”‚
â”‚   Una COLECCIÃ“N de:                                    â”‚
â”‚                                                         â”‚
â”‚   â”œâ”€â”€ Prompts que tu sistema procesÃ³ anteriormente     â”‚
â”‚   â””â”€â”€ InformaciÃ³n sobre el journey de ese prompt       â”‚
â”‚                                                         â”‚
â”‚   Puede ser simple (prompt + response) o detallado     â”‚
â”‚   (todos los pasos del pipeline).                      â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Â¿QuÃ© Datos Guardar?

### La pregunta clave:

```
Â¿QuÃ© querÃ©s EVALUAR?

La respuesta determina quÃ© datos guardar.
```

### Nivel MÃ­nimo (End-to-End):

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                         â”‚
â”‚   DATOS BÃSICOS:                                       â”‚
â”‚   â”œâ”€â”€ Input prompt del usuario                         â”‚
â”‚   â””â”€â”€ Respuesta final del sistema                      â”‚
â”‚                                                         â”‚
â”‚   PERMITE:                                             â”‚
â”‚   â”œâ”€â”€ Sentido general de performance                   â”‚
â”‚   â””â”€â”€ Trackear cÃ³mo cambian respuestas con edits       â”‚
â”‚                                                         â”‚
â”‚   LIMITACIÃ“N:                                          â”‚
â”‚   Solo evaluaciÃ³n end-to-end, no component-level.      â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Nivel Detallado (Component-Level):

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                         â”‚
â”‚   PARA EVALUACIÃ“N POR COMPONENTE, guardar:             â”‚
â”‚                                                         â”‚
â”‚   INPUT/OUTPUT de cada componente:                     â”‚
â”‚   â”œâ”€â”€ Query rewriter (input â†’ output)                 â”‚
â”‚   â”œâ”€â”€ Retriever (query â†’ docs encontrados)            â”‚
â”‚   â”œâ”€â”€ Re-ranker (docs antes â†’ docs despuÃ©s)           â”‚
â”‚   â”œâ”€â”€ Router LLM (input â†’ decisiÃ³n)                   â”‚
â”‚   â””â”€â”€ Generator LLM (prompt â†’ response)               â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Estructura TÃ­pica de Dataset

### Tabla con docenas de columnas:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                         â”‚
â”‚   METADATA:                                            â”‚
â”‚   â”œâ”€â”€ request_id                                       â”‚
â”‚   â”œâ”€â”€ timestamp                                        â”‚
â”‚   â”œâ”€â”€ customer_id                                      â”‚
â”‚   â”œâ”€â”€ session_id                                       â”‚
â”‚   â””â”€â”€ topic/category                                   â”‚
â”‚                                                         â”‚
â”‚   INPUT:                                               â”‚
â”‚   â”œâ”€â”€ original_prompt                                  â”‚
â”‚   â””â”€â”€ conversation_history                             â”‚
â”‚                                                         â”‚
â”‚   QUERY PROCESSING:                                    â”‚
â”‚   â”œâ”€â”€ rewritten_query                                  â”‚
â”‚   â”œâ”€â”€ expanded_queries                                 â”‚
â”‚   â””â”€â”€ detected_intent                                  â”‚
â”‚                                                         â”‚
â”‚   RETRIEVAL:                                           â”‚
â”‚   â”œâ”€â”€ docs_retrieved (IDs)                            â”‚
â”‚   â”œâ”€â”€ retrieval_scores                                 â”‚
â”‚   â”œâ”€â”€ docs_after_rerank                               â”‚
â”‚   â””â”€â”€ rerank_scores                                   â”‚
â”‚                                                         â”‚
â”‚   GENERATION:                                          â”‚
â”‚   â”œâ”€â”€ final_prompt_to_llm                             â”‚
â”‚   â”œâ”€â”€ llm_response                                    â”‚
â”‚   â””â”€â”€ tokens_used                                     â”‚
â”‚                                                         â”‚
â”‚   PERFORMANCE:                                         â”‚
â”‚   â”œâ”€â”€ total_latency_ms                                â”‚
â”‚   â”œâ”€â”€ retrieval_latency_ms                            â”‚
â”‚   â”œâ”€â”€ llm_latency_ms                                  â”‚
â”‚   â””â”€â”€ cost_usd                                        â”‚
â”‚                                                         â”‚
â”‚   FEEDBACK:                                            â”‚
â”‚   â”œâ”€â”€ user_rating (thumbs up/down)                    â”‚
â”‚   â””â”€â”€ user_comments                                   â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Beneficios de Datos Detallados

### AnÃ¡lisis multidimensional:

```
Con datos detallados podÃ©s:

1. FILTRAR por dimensiÃ³n
   â”œâ”€â”€ Por topic de pregunta
   â”œâ”€â”€ Por tipo de cliente
   â”œâ”€â”€ Por perÃ­odo de tiempo
   â””â”€â”€ Por intent detectado

2. DETECTAR patrones
   â”œâ”€â”€ "Preguntas sobre refunds â†’ alta calidad"
   â”œâ”€â”€ "Preguntas sobre delays â†’ baja calidad"
   â””â”€â”€ "Ciertos clientes â†’ mÃ¡s problemas"

3. INVESTIGAR causas
   â”œâ”€â”€ Â¿El retriever no encuentra docs?
   â”œâ”€â”€ Â¿El re-ranker elimina docs buenos?
   â””â”€â”€ Â¿El LLM ignora el contexto?
```

---

## Caso Real: Diagrams vs Images

### El problema:

```
Sistema RAG que genera:
â”œâ”€â”€ Texto
â”œâ”€â”€ ImÃ¡genes (text-to-image)
â””â”€â”€ Charts/diagrams (Mermaid.js code)

QUEJA: "La calidad de algunos diagramas es muy baja"
```

### Debugging con logs:

```
Trabajando hacia atrÃ¡s en los logs:

1. Encontrar prompts con mala calidad de diagramas
2. Analizar el journey de esos prompts
3. DESCUBRIMIENTO: Usuario pedÃ­a "draw a diagram"
4. Router LLM interpretÃ³ "draw" â†’ imagen
5. EnviÃ³ a text-to-image model
6. Text-to-image es MALO para charts

SOLUCIÃ“N: Actualizar system prompt del router
para que "diagram" â†’ Mermaid.js, no image generation
```

### El valor del logging:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                         â”‚
â”‚   Gracias a un sistema robusto de monitoring y logging:â”‚
â”‚                                                         â”‚
â”‚   1. Recibimos reportes de clientes                    â”‚
â”‚   2. FÃ¡cil trackear la fuente del problema             â”‚
â”‚   3. Fix rÃ¡pido a producciÃ³n                           â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## VisualizaciÃ³n de Datos

### Clustering de prompts:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                         â”‚
â”‚   Visualizar TODOS los prompts del sistema:            â”‚
â”‚                                                         â”‚
â”‚   1. Embeber prompts                                   â”‚
â”‚   2. Clustering algorithm (k-means, etc.)              â”‚
â”‚   3. Identificar topics de alto nivel:                 â”‚
â”‚                                                         â”‚
â”‚      â—‹ â—‹ â—‹ â—‹ â—‹   Cluster A: Product questions          â”‚
â”‚        â—‹ â—‹ â—‹                                           â”‚
â”‚                                                         â”‚
â”‚            â—‹ â—‹ â—‹ â—‹   Cluster B: Troubleshooting       â”‚
â”‚          â—‹ â—‹ â—‹ â—‹ â—‹                                     â”‚
â”‚                                                         â”‚
â”‚      â—‹ â—‹             Cluster C: Refunds               â”‚
â”‚        â—‹ â—‹ â—‹                                           â”‚
â”‚                                                         â”‚
â”‚   4. Correr evals SOLO en un cluster                   â”‚
â”‚   5. Detectar si ciertos topics underperforman         â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## AplicaciÃ³n para DONA ğŸ¯

### Estructura de dataset para DONA:

```python
DONA_DATASET_SCHEMA = {
    # Metadata
    "request_id": str,
    "timestamp": datetime,
    "session_id": str,
    "channel": str,  # web, whatsapp, etc.
    
    # Input
    "user_message": str,
    "conversation_history": list,
    
    # Intent & Query Processing
    "detected_intent": str,  # consulta_precio, consulta_stock, saludo, etc.
    "rewritten_query": str,
    
    # Retrieval
    "products_retrieved": list,  # IDs de productos
    "retrieval_scores": list,
    "products_after_rerank": list,
    
    # Generation
    "prompt_to_llm": str,
    "response": str,
    "tokens_input": int,
    "tokens_output": int,
    
    # Productos mencionados en respuesta
    "products_mentioned": list,
    "prices_mentioned": list,
    
    # Performance
    "total_latency_ms": int,
    "retrieval_latency_ms": int,
    "llm_latency_ms": int,
    "cost_usd": float,
    
    # Quality signals
    "grounding_warnings": list,
    "price_accuracy": bool,  # Â¿Precios coinciden con DB?
    
    # Feedback
    "user_rating": str,  # thumbs_up, thumbs_down, none
    "led_to_sale": bool,  # Si terminÃ³ en venta
}
```

### AnÃ¡lisis por cluster para DONA:

```python
def analyze_dona_clusters():
    """
    Identificar clusters de preguntas y performance por cluster
    """
    
    # Posibles clusters en DONA:
    clusters = {
        "consulta_precio": {
            "example": "Â¿CuÃ¡nto sale el cemento?",
            "expected_performance": "high",
            "key_metric": "price_accuracy"
        },
        "consulta_stock": {
            "example": "Â¿Tienen hierro del 8?",
            "expected_performance": "high",
            "key_metric": "availability_accuracy"
        },
        "recomendacion": {
            "example": "Â¿QuÃ© me recomendÃ¡s para una losa?",
            "expected_performance": "medium",
            "key_metric": "response_relevancy"
        },
        "calculo": {
            "example": "Â¿CuÃ¡nto material necesito para 20mÂ²?",
            "expected_performance": "medium",
            "key_metric": "calculation_accuracy"
        },
        "fuera_dominio": {
            "example": "Â¿Venden comida para perros?",
            "expected_performance": "should_decline",
            "key_metric": "proper_decline_rate"
        }
    }
    
    # Evaluar cada cluster por separado
    for cluster_name, config in clusters.items():
        prompts = filter_by_cluster(dataset, cluster_name)
        metrics = evaluate_cluster(prompts, config["key_metric"])
        
        if metrics < config["expected_performance"]:
            alert(f"Cluster {cluster_name} underperforming")
```

### Caso de debugging para DONA:

```
PROBLEMA REPORTADO:
"A veces DONA da precios incorrectos"

INVESTIGACIÃ“N CON LOGS:
1. Filtrar respuestas donde price_accuracy = False
2. Analizar los prompts
3. DESCUBRIMIENTO: Muchos son preguntas ambiguas
   "Â¿CuÃ¡nto sale el cemento?" sin especificar marca
4. Retriever trae VARIOS cementos
5. LLM a veces menciona precio de uno pero nombre de otro

SOLUCIÃ“N:
- Actualizar system prompt para siempre confirmar producto
- O: Cuando hay mÃºltiples matches, preguntar cuÃ¡l
```

---

## Resumen del CapÃ­tulo 42

| Aspecto | RecomendaciÃ³n |
|---------|---------------|
| **QuÃ© guardar** | Lo que querÃ©s evaluar |
| **MÃ­nimo** | Input prompt + response final |
| **Ideal** | Input/output de CADA componente |
| **AnÃ¡lisis** | Filtrar por dimensiones, clustering |
| **Valor** | Debugging rÃ¡pido, mejora continua |

---

## Key Takeaways:

```
1. Custom datasets = prompts REALES que tu sistema procesÃ³

2. Guardar datos de CADA componente para debugging granular

3. Permite anÃ¡lisis MULTIDIMENSIONAL
   (por topic, por cliente, por tiempo)

4. VISUALIZAR prompts con clustering para detectar
   patrones de underperformance

5. El mejor way de mejorar = testear con prompts REALES
   de tus usuarios
```

---

## PrÃ³ximo: Trade-offs de RAG en ProducciÃ³n

Balancear costo, latencia, y calidad.

---
