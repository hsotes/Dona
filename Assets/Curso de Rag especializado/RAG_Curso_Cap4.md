# CapÃ­tulo 4: Arquitectura de un Sistema RAG

---

## LLM Directo vs RAG

### Uso normal de un LLM:

```
Usuario â†’ Prompt â†’ LLM â†’ Respuesta
```

### Uso con RAG:

```
Usuario â†’ Prompt â†’ [RETRIEVER â†’ Knowledge Base] â†’ Augmented Prompt â†’ LLM â†’ Respuesta
```

> "La experiencia del usuario es idÃ©ntica. EnviÃ¡s un prompt, recibÃ­s una respuesta. Internamente, hay algunos pasos mÃ¡s."

---

## El Flujo Completo de RAG

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                         â”‚
â”‚   1. USUARIO ENVÃA PROMPT                              â”‚
â”‚      "Â¿Por quÃ© los hoteles en Vancouver estÃ¡n          â”‚
â”‚       tan caros este fin de semana?"                   â”‚
â”‚                          â”‚                              â”‚
â”‚                          â–¼                              â”‚
â”‚   2. RETRIEVER BUSCA EN KNOWLEDGE BASE                 â”‚
â”‚      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
â”‚      â”‚      KNOWLEDGE BASE            â”‚                â”‚
â”‚      â”‚  (database de documentos)      â”‚                â”‚
â”‚      â”‚                                â”‚                â”‚
â”‚      â”‚  â€¢ ArtÃ­culo sobre Taylor Swift â”‚                â”‚
â”‚      â”‚  â€¢ Noticias de Vancouver       â”‚                â”‚
â”‚      â”‚  â€¢ Info de hoteles             â”‚                â”‚
â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
â”‚                          â”‚                              â”‚
â”‚                          â–¼                              â”‚
â”‚      Retriever devuelve: "5 artÃ­culos relevantes"      â”‚
â”‚                          â”‚                              â”‚
â”‚                          â–¼                              â”‚
â”‚   3. SE CREA AUGMENTED PROMPT                          â”‚
â”‚      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
â”‚      â”‚ "Answer the following question: â”‚                â”‚
â”‚      â”‚  Â¿Por quÃ© los hoteles en        â”‚                â”‚
â”‚      â”‚  Vancouver estÃ¡n tan caros?     â”‚                â”‚
â”‚      â”‚                                 â”‚                â”‚
â”‚      â”‚  Here are 5 relevant articles   â”‚                â”‚
â”‚      â”‚  that may help you respond:     â”‚                â”‚
â”‚      â”‚  [texto de los artÃ­culos]"      â”‚                â”‚
â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
â”‚                          â”‚                              â”‚
â”‚                          â–¼                              â”‚
â”‚   4. LLM GENERA RESPUESTA                              â”‚
â”‚      (usando conocimiento de training                  â”‚
â”‚       + contexto de documentos recuperados)            â”‚
â”‚                          â”‚                              â”‚
â”‚                          â–¼                              â”‚
â”‚   5. USUARIO RECIBE RESPUESTA                          â”‚
â”‚      "Los hoteles estÃ¡n caros porque Taylor Swift      â”‚
â”‚       tiene un show de dos noches este weekend..."     â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## La Diferencia Clave

```
SIN RAG:                    CON RAG:
                            
Prompt â†’ LLM â†’ Response     Prompt â†’ Retriever â†’ Augmented â†’ LLM â†’ Response
                                         â”‚          Prompt
                                         â–¼
                                   Knowledge
                                     Base
```

> "La principal diferencia entre usar un LLM directamente y un sistema RAG es la adiciÃ³n del retriever."

---

## Ventajas de RAG

### 1. Acceso a informaciÃ³n que el LLM no tiene

```
â”œâ”€â”€ PolÃ­ticas de tu empresa
â”œâ”€â”€ InformaciÃ³n personal
â”œâ”€â”€ Noticias de esta maÃ±ana
â””â”€â”€ Cualquier dato privado o reciente
```

> "RAG es frecuentemente la ÃšNICA forma de hacer disponible ciertos tipos de informaciÃ³n a un LLM."

---

### 2. Reduce alucinaciones

```
Causa de alucinaciones:
â”œâ”€â”€ Temas excluidos del training
â”œâ”€â”€ Temas mencionados raramente
â””â”€â”€ LLM "inventa" para llenar gaps

SoluciÃ³n RAG:
â”œâ”€â”€ InformaciÃ³n directamente en el prompt
â”œâ”€â”€ Respuestas FUNDAMENTADAS en docs reales
â””â”€â”€ Menos texto genÃ©rico o engaÃ±oso
```

---

### 3. FÃ¡cil de actualizar

```
REENTRENAR LLM:              ACTUALIZAR RAG:
â”œâ”€â”€ Costoso                  â”œâ”€â”€ Barato
â”œâ”€â”€ Lento                    â”œâ”€â”€ RÃ¡pido
â””â”€â”€ Complejo                 â””â”€â”€ Simple (como actualizar DB)

Knowledge Base actualizada â†’ LLM responde con info nueva
```

> "PodÃ©s simplemente actualizar la informaciÃ³n en la knowledge base, igual que actualizarÃ­as entradas en cualquier otra base de datos."

---

### 4. Permite citar fuentes

```
Augmented Prompt:
"[Documento: Manual de RRHH, pÃ¡g 15]
 La polÃ­tica de vacaciones es..."

Respuesta del LLM:
"SegÃºn el Manual de RRHH (pÃ¡g 15), 
 la polÃ­tica de vacaciones es..."

â†’ El usuario puede verificar la fuente
```

---

### 5. DivisiÃ³n de responsabilidades

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                         â”‚
â”‚   RETRIEVER:                  LLM:                     â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                 â”€â”€â”€â”€                     â”‚
â”‚   â€¢ Filtrar informaciÃ³n       â€¢ Generar texto          â”‚
â”‚   â€¢ Encontrar lo relevante    â€¢ Escribir respuesta     â”‚
â”‚   â€¢ Presentar sucintamente    â€¢ Razonar sobre contexto â”‚
â”‚                                                         â”‚
â”‚   "Cada componente trabaja en su Ã¡rea de              â”‚
â”‚    mayor fortaleza."                                   â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## CÃ³digo BÃ¡sico de RAG

### Las dos funciones principales:

```python
def retrieve(query: str) -> list:
    """
    Wrapper del retriever.
    Acepta un query de texto.
    Retorna documentos relevantes de la knowledge base.
    """
    return retriever.search(query)


def generate(prompt: str) -> str:
    """
    Wrapper del LLM.
    Acepta un prompt de texto.
    Retorna la respuesta del LLM.
    """
    return llm.complete(prompt)
```

---

### Flujo completo:

```python
# 1. El prompt del usuario
prompt = "Â¿Por quÃ© los hoteles en Vancouver estÃ¡n tan caros este weekend?"

# 2. SIN RAG - respuesta directa del LLM
response_without_rag = generate(prompt)
# Resultado: respuesta genÃ©rica, posiblemente incorrecta

# 3. Recuperar informaciÃ³n relevante
retrieved_docs = retrieve(prompt)
# Resultado: artÃ­culos sobre Taylor Swift en Vancouver

# 4. Crear augmented prompt
augmented_prompt = f"""
Respond to the following prompt:
{prompt}

Using the following information retrieved to help you answer:
{retrieved_docs}
"""

# 5. CON RAG - respuesta con contexto
response_with_rag = generate(augmented_prompt)
# Resultado: "Los hoteles estÃ¡n caros porque Taylor Swift 
#             tiene un show este weekend..."
```

---

## Resumen Visual

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    SISTEMA RAG                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  PROMPT  â”‚â”€â”€â”€â–ºâ”‚RETRIEVER â”‚â”€â”€â”€â–ºâ”‚ AUGMENTED PROMPT â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                       â”‚                    â”‚            â”‚
â”‚                       â–¼                    â”‚            â”‚
â”‚                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚            â”‚
â”‚                 â”‚KNOWLEDGE â”‚               â”‚            â”‚
â”‚                 â”‚  BASE    â”‚               â”‚            â”‚
â”‚                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚            â”‚
â”‚                                            â–¼            â”‚
â”‚                                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚                                      â”‚   LLM    â”‚       â”‚
â”‚                                      â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜       â”‚
â”‚                                           â”‚             â”‚
â”‚                                           â–¼             â”‚
â”‚                                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚                                      â”‚ RESPONSE â”‚       â”‚
â”‚                                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Ventajas - Resumen

| Ventaja | ExplicaciÃ³n |
|---------|-------------|
| **Acceso a info** | Datos que LLM no conoce |
| **Menos alucinaciones** | Respuestas fundamentadas |
| **FÃ¡cil actualizar** | Solo cambiar knowledge base |
| **Citar fuentes** | Usuario puede verificar |
| **DivisiÃ³n de trabajo** | Cada componente en su fortaleza |

---

## AplicaciÃ³n para DONA ğŸ¯

### Tu flujo deberÃ­a ser:

```python
# Usuario pregunta
prompt = "Â¿CuÃ¡nto sale el cemento Portland?"

# DONA recupera del catÃ¡logo
docs = retrieve(prompt)
# â†’ "Cemento Portland 50kg - $8500 - Stock: 150 bolsas"

# Augmented prompt
augmented = f"""
Sos DONA, asistente de Materiales Boto Mariani.
Pregunta: {prompt}
InformaciÃ³n del catÃ¡logo: {docs}
RespondÃ© basÃ¡ndote SOLO en la informaciÃ³n del catÃ¡logo.
"""

# LLM genera respuesta fundamentada
response = generate(augmented)
# â†’ "El cemento Portland de 50kg sale $8500. 
#    Tenemos 150 bolsas en stock."
```

---

## PrÃ³ximo: Deep Dive en cada Componente

Vamos a ver en detalle el Retriever, la Knowledge Base, y el LLM.

---
