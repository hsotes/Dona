# CapÃ­tulo 40: Sistema de Observabilidad

---

## La Base de ProducciÃ³n

> "Un buen primer paso para manejar los desafÃ­os de producciÃ³n es construir un sistema de observabilidad robusto."

---

## QuÃ© Debe Trackear una Plataforma de Observabilidad

### 1. MÃ©tricas de Performance de Software

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                         â”‚
â”‚   Las mÃ©tricas clÃ¡sicas de cualquier sistema:          â”‚
â”‚                                                         â”‚
â”‚   â”œâ”€â”€ LATENCIA (tiempo de respuesta)                   â”‚
â”‚   â”œâ”€â”€ THROUGHPUT (requests por segundo)                â”‚
â”‚   â”œâ”€â”€ MEMORIA (uso de RAM)                             â”‚
â”‚   â””â”€â”€ COMPUTE (uso de CPU/GPU)                         â”‚
â”‚                                                         â”‚
â”‚   Â¿CuÃ¡ntos requests maneja?                            â”‚
â”‚   Â¿CuÃ¡nto tarda?                                       â”‚
â”‚   Â¿CuÃ¡ntos recursos consume?                           â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2. MÃ©tricas de Calidad

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                         â”‚
â”‚   MÃ¡s allÃ¡ de velocidad/eficiencia:                    â”‚
â”‚   Â¿Los resultados cumplen los estÃ¡ndares de calidad?   â”‚
â”‚                                                         â”‚
â”‚   â”œâ”€â”€ SatisfacciÃ³n del usuario con respuestas         â”‚
â”‚   â”œâ”€â”€ Recall del retriever                             â”‚
â”‚   â”œâ”€â”€ Faithfulness del LLM                            â”‚
â”‚   â””â”€â”€ PrecisiÃ³n de citas                              â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 3. EstadÃ­sticas Agregadas

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                         â”‚
â”‚   Datos agregados OVER TIME:                           â”‚
â”‚                                                         â”‚
â”‚   â”œâ”€â”€ Trackear tendencias de alto nivel               â”‚
â”‚   â”œâ”€â”€ Identificar regresiones rÃ¡pidamente             â”‚
â”‚   â””â”€â”€ Comparar perÃ­odos (semana vs semana)            â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 4. Logs Detallados

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                         â”‚
â”‚   Logs para TRACING individual:                        â”‚
â”‚                                                         â”‚
â”‚   Seguir el journey de prompts individuales            â”‚
â”‚   a travÃ©s del pipeline RAG.                           â”‚
â”‚                                                         â”‚
â”‚   Ãštil para entender la FUENTE de                      â”‚
â”‚   respuestas de bajo rendimiento.                      â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 5. ExperimentaciÃ³n

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                         â”‚
â”‚   Habilitar EXPERIMENTACIÃ“N:                           â”‚
â”‚                                                         â”‚
â”‚   â”œâ”€â”€ Testear nuevo modelo de lenguaje                â”‚
â”‚   â”œâ”€â”€ Ajustar system prompt                           â”‚
â”‚   â”œâ”€â”€ Cambiar settings del retriever                  â”‚
â”‚                                                         â”‚
â”‚   Opciones:                                            â”‚
â”‚   â”œâ”€â”€ Experimentos en ambiente seguro                 â”‚
â”‚   â””â”€â”€ A/B testing con usuarios en producciÃ³n          â”‚
â”‚                                                         â”‚
â”‚   Monitorear impacto en mÃ©tricas de                   â”‚
â”‚   performance y calidad para decidir si deployar.     â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Framework: Scope Ã— Evaluator Type

### Las dos dimensiones:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                         â”‚
â”‚   SCOPE (Alcance):                                     â”‚
â”‚   â”œâ”€â”€ SYSTEM-LEVEL: Performance general               â”‚
â”‚   â””â”€â”€ COMPONENT-LEVEL: Debug de issues especÃ­ficos    â”‚
â”‚                                                         â”‚
â”‚   EVALUATOR TYPE (Tipo de evaluador):                  â”‚
â”‚   â”œâ”€â”€ CODE-BASED: AutomÃ¡tico, determinÃ­stico, barato  â”‚
â”‚   â”œâ”€â”€ LLM-AS-JUDGE: Flexible, medio costo            â”‚
â”‚   â””â”€â”€ HUMAN FEEDBACK: Costoso pero captura mÃ¡s       â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### El Grid:

```
                    â”‚ Code-Based â”‚ LLM-as-Judge â”‚ Human Feedback â”‚
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
SYSTEM-LEVEL        â”‚ Latency    â”‚ Response     â”‚ Thumbs up/down â”‚
                    â”‚ Throughput â”‚ quality      â”‚ User comments  â”‚
                    â”‚ Memory     â”‚              â”‚                â”‚
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
COMPONENT-LEVEL     â”‚ LLM tokens â”‚ Faithfulness â”‚ Annotated      â”‚
(Retriever)         â”‚ per second â”‚ Relevancy    â”‚ test datasets  â”‚
(LLM)               â”‚ JSON valid â”‚              â”‚ (precision/    â”‚
(etc.)              â”‚            â”‚              â”‚  recall)       â”‚
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Scope: System vs Component

### System-Level:

```
PROPÃ“SITO:
â”œâ”€â”€ Resumir performance general
â”œâ”€â”€ Vista de alto nivel de cÃ³mo van las cosas
â””â”€â”€ Dashboards ejecutivos

EJEMPLOS:
â”œâ”€â”€ Latencia promedio del sistema
â”œâ”€â”€ Throughput total
â”œâ”€â”€ User satisfaction rate
â””â”€â”€ Error rate general
```

### Component-Level:

```
PROPÃ“SITO:
â”œâ”€â”€ Debugear fuente de issues especÃ­ficos
â””â”€â”€ Identificar quÃ© componente causa problemas

EJEMPLO:
Sistema tiene latencia alta â†’
Â¿Es el retriever? Â¿El LLM? Â¿Otro componente?

NecesitÃ¡s mÃ©tricas por componente para saber.
```

---

## Evaluator Types

### 1. Code-Based Evals

```
âœ… MÃ¡s baratos
âœ… MÃ¡s simples
âœ… MÃ¡s directos de implementar
âœ… AutomÃ¡ticos
âœ… DeterminÃ­sticos
âœ… Casi gratis de correr

EJEMPLOS:
â”œâ”€â”€ Requests por segundo
â”œâ”€â”€ Latencia en ms
â”œâ”€â”€ Memoria usada
â”œâ”€â”€ Unit tests (Â¿output es JSON vÃ¡lido?)
â”œâ”€â”€ Tokens generados por segundo
â””â”€â”€ Error rates
```

### 2. Human Feedback

```
âŒ MÃ¡s costoso
âœ… Captura info que code-based no puede

EJEMPLOS:
â”œâ”€â”€ Thumbs up/down en respuestas
â”œâ”€â”€ Text boxes para feedback detallado
â”œâ”€â”€ Datasets pre-compilados por humanos
â”‚   (prompt + documentos relevantes esperados)
â””â”€â”€ Anotaciones de calidad

NOTA: Algunos evals corren automÃ¡ticamente
pero DEPENDEN de input humano inicial
(ej: el dataset de test lo compila un humano)
```

### 3. LLM-as-a-Judge

```
âœ… MÃ¡s flexible que code-based
âœ… MÃ¡s barato que human feedback
âš ï¸ Necesita ser cuidadosamente tuneado

CUIDADOS:
â”œâ”€â”€ Modelos tienen BIASES
â”‚   (favorecen respuestas de su propia familia)
â”œâ”€â”€ Necesitan RUBRICS claros
â””â”€â”€ Funcionan mejor con estÃ¡ndares DISCRETOS
    (relevant/irrelevant) 
    vs escalas continuas (0-100)

EJEMPLOS:
â”œâ”€â”€ Â¿Documentos recuperados son relevantes?
â”œâ”€â”€ Response relevancy (RAGAS)
â”œâ”€â”€ Faithfulness (RAGAS)
â””â”€â”€ Citation quality
```

---

## Set Inicial de MÃ©tricas Recomendado

### Performance Metrics (Code-Based):

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                         â”‚
â”‚   POR COMPONENTE Y SISTEMA:                            â”‚
â”‚                                                         â”‚
â”‚   â”œâ”€â”€ Latencia (ms)                                    â”‚
â”‚   â”œâ”€â”€ Throughput (requests/segundo)                    â”‚
â”‚   â”œâ”€â”€ Memory usage (MB)                                â”‚
â”‚   â””â”€â”€ Tokens generados por segundo                     â”‚
â”‚                                                         â”‚
â”‚   Son BARATAS y FÃCILES de colectar.                  â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Quality Metrics:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                         â”‚
â”‚   SYSTEM-WIDE (Human Feedback):                        â”‚
â”‚   â”œâ”€â”€ Thumbs up/down en respuestas                     â”‚
â”‚   â””â”€â”€ Feedback rate general                            â”‚
â”‚                                                         â”‚
â”‚   RETRIEVER (Human Annotated Dataset):                 â”‚
â”‚   â”œâ”€â”€ Recall                                           â”‚
â”‚   â”œâ”€â”€ Precision                                        â”‚
â”‚   â””â”€â”€ MRR                                              â”‚
â”‚                                                         â”‚
â”‚   LLM (LLM-as-Judge / RAGAS):                         â”‚
â”‚   â”œâ”€â”€ Response relevancy                               â”‚
â”‚   â”œâ”€â”€ Faithfulness                                     â”‚
â”‚   â”œâ”€â”€ Citation quality                                 â”‚
â”‚   â””â”€â”€ Noise sensitivity                                â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## AplicaciÃ³n para DONA ğŸ¯

### Dashboard de MÃ©tricas para DONA:

```python
DONA_OBSERVABILITY = {
    # Performance (Code-Based) - Barato
    "performance": {
        "system": {
            "latency_p50_ms": target < 2000,
            "latency_p95_ms": target < 5000,
            "throughput_rps": monitor,
            "error_rate": target < 0.01
        },
        "retriever": {
            "search_latency_ms": target < 500,
            "docs_retrieved_avg": monitor
        },
        "llm": {
            "generation_latency_ms": target < 1500,
            "tokens_per_second": monitor,
            "cost_per_query_usd": monitor
        }
    },
    
    # Quality - MÃ¡s costoso pero necesario
    "quality": {
        "system": {
            "thumbs_up_rate": target > 0.85,
            "thumbs_down_rate": alert_if > 0.15
        },
        "retriever": {
            "recall_at_5": target > 0.80,
            "precision_at_5": target > 0.70
        },
        "llm": {
            "response_relevancy": target > 0.85,
            "faithfulness": target > 0.90,
            "price_accuracy": target > 0.95  # Custom para DONA
        }
    }
}
```

### Logs para Tracing en DONA:

```python
def log_dona_request(request_id, query, retrieved_docs, response):
    """Log detallado para debugging"""
    log_entry = {
        "request_id": request_id,
        "timestamp": datetime.now(),
        
        # Input
        "user_query": query,
        "query_after_rewriting": rewritten_query,
        
        # Retrieval
        "docs_retrieved": len(retrieved_docs),
        "retrieval_latency_ms": retrieval_time,
        "top_doc_score": retrieved_docs[0]['score'] if retrieved_docs else None,
        
        # LLM
        "llm_latency_ms": llm_time,
        "tokens_generated": token_count,
        "model_used": model_name,
        
        # Output
        "response_length": len(response),
        "mentioned_products": extract_products(response),
        "mentioned_prices": extract_prices(response),
        
        # Quality indicators
        "grounding_warnings": check_grounding(response, retrieved_docs)
    }
    
    save_log(log_entry)
    
    # Alert si hay problemas
    if log_entry["grounding_warnings"]:
        alert_for_review(log_entry)
```

### A/B Testing Setup para DONA:

```python
def dona_ab_test(experiment_name, variant_a_config, variant_b_config):
    """
    A/B test de cambios en DONA
    
    Ejemplos de experimentos:
    - Nuevo system prompt
    - Diferente modelo LLM
    - Cambio en alpha de hybrid search
    - Nuevo re-ranker
    """
    
    for request in incoming_requests:
        # 50/50 split
        variant = "A" if hash(request.user_id) % 2 == 0 else "B"
        config = variant_a_config if variant == "A" else variant_b_config
        
        # Process with variant config
        response = process_with_config(request, config)
        
        # Log variant for analysis
        log_experiment(experiment_name, variant, request, response)
    
    # DespuÃ©s de N requests, analizar:
    # - Latencia por variante
    # - Thumbs up rate por variante
    # - Costo por variante
    # - Quality metrics por variante
```

---

## Resumen del CapÃ­tulo 40

| Tipo de Eval | Costo | QuÃ© Captura | Ejemplo |
|--------------|-------|-------------|---------|
| **Code-Based** | Bajo | Performance | Latencia, throughput |
| **LLM-as-Judge** | Medio | Calidad flexible | Relevancy, faithfulness |
| **Human** | Alto | Calidad real | Thumbs up/down, datasets |

---

## Key Takeaways:

```
1. Trackear PERFORMANCE (barato) + QUALITY (mÃ¡s costoso)

2. MÃ©tricas a nivel SISTEMA + nivel COMPONENTE

3. EstadÃ­sticas AGREGADAS para tendencias
   + LOGS detallados para debugging

4. Habilitar EXPERIMENTACIÃ“N (A/B testing)

5. Balance entre evals baratos (code-based)
   y costosos pero informativos (human/LLM)
```

---

## PrÃ³ximo: ImplementaciÃ³n de Observabilidad

Consideraciones al implementar este sistema.

---
